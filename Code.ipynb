{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36d76df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pgmpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87c80f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dded4038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# === Import Preamble ===\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "import colorsys\n",
    "import itertools\n",
    "from itertools import combinations\n",
    "from math import log2\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from pyvis.network import Network\n",
    "from graphviz import Digraph\n",
    "\n",
    "from pgmpy.models import DiscreteBayesianNetwork\n",
    "from pgmpy.factors.discrete import TabularCPD, DiscreteFactor\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "# ============================================================================\n",
    "g = Digraph(format=\"pdf\", graph_attr={\"rankdir\":\"LR\", \"size\":\"11.69,8.27!\"})\n",
    "g.edge(\"root\",\"child\", label=\"AS: Splatterâ†’Pass\\n0.40\")\n",
    "g.render(\"tiny_test\", cleanup=True)  # writes tiny_test.pdf\n",
    "\n",
    "# Optional: Graphviz layout availability flag\n",
    "try:\n",
    "    from networkx.drawing.nx_pydot import graphviz_layout as gv_layout\n",
    "    _HAS_GV = True\n",
    "except Exception:\n",
    "    _HAS_GV = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ced7c75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants & Helpers \n",
    "random.seed(0)\n",
    "\n",
    "HW_MIN, HW_MAX = 0.60, 0.95    # caps for handwriting reliability\n",
    "EPS            = 1e-6          # small mass to avoid zeros\n",
    "\n",
    "H_STATES = [\"h1\",\"h2\",\"h3\",\"h4\",\"h5\"]\n",
    "AUTHOR_CODES = [\"a1\",\"a2\",\"a3\",\"a4\"]\n",
    "\n",
    "def normalise(dist, eps=EPS):\n",
    "    \"\"\"Replace zeros with eps, then renormalise to sum=1.\"\"\"\n",
    "    r = [x if x > 0 else eps for x in dist]\n",
    "    S = sum(r)\n",
    "    return [x / S for x in r]\n",
    "\n",
    "def make_prior_dict(keys, values):\n",
    "    \"\"\"Build a prior dict and assert it sums to 1.\"\"\"\n",
    "    d = dict(zip(keys, values))\n",
    "    assert abs(sum(d.values()) - 1.0) < 1e-8, \"Priors must sum to 1\"\n",
    "    return d\n",
    "\n",
    "def override_mode_timing(h_list, tup):\n",
    "    \"\"\"Bulk override mode_timing_pr for all modes in h_list with tuple tup.\"\"\"\n",
    "    for h in h_list:\n",
    "        mode_timing_pr[h] = tup\n",
    "\n",
    "def override_timing_author(h_list, t, tup):\n",
    "    \"\"\"Bulk override timing_author_pr[(h,t)] for h in h_list with tuple tup.\"\"\"\n",
    "    for h in h_list:\n",
    "        timing_author_pr[(h, t)] = tup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc7f8389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode-of-death priors (scenarios) \n",
    "mode_keys           = [f\"h{i}\" for i in range(1, 6)]\n",
    "mode_pr_values      = [0.10, 0.08, 0.06, 0.46, 0.30]\n",
    "mode_pr_biased_vals = [0.25, 0.04, 0.04, 0.52, 0.15]\n",
    "mode_pr_fair_vals   = [0.10, 0.10, 0.10, 0.60, 0.10]\n",
    "mode_pr_extreme_vals= [0.40, 0.05, 0.05, 0.40, 0.10]\n",
    "\n",
    "mode_pr        = make_prior_dict(mode_keys, mode_pr_values)\n",
    "mode_pr_biased = make_prior_dict(mode_keys, mode_pr_biased_vals)\n",
    "mode_pr_fair   = make_prior_dict(mode_keys, mode_pr_fair_vals)\n",
    "mode_pr_extreme= make_prior_dict(mode_keys, mode_pr_extreme_vals)\n",
    "\n",
    "prs = {\n",
    "    'True-world':        mode_pr,\n",
    "    'Investigator-bias': mode_pr_biased,\n",
    "    'Fair-reweighted':   mode_pr_fair,\n",
    "    'Extreme-bias':      mode_pr_extreme,\n",
    "}\n",
    "bias_scenarios = list(prs.items())\n",
    "\n",
    "# PARAMETER DEFINITIONS \n",
    "\n",
    "# 1a) Mode-of-death priors (duplicate kept; original order preserved)\n",
    "mode_pr = {\n",
    "    \"h1\": 0.10,\n",
    "    \"h2\": 0.08,\n",
    "    \"h3\": 0.06,\n",
    "    \"h4\": 0.46,\n",
    "    \"h5\": 0.30,\n",
    "}\n",
    "\n",
    "# 1b) Letter timing CPTs: (P(before), P(after))\n",
    "mode_timing_pr = {\n",
    "    \"h1\": (0.90, 0.10),\n",
    "    \"h2\": (0.30, 0.70),\n",
    "    \"h3\": (0.30, 0.70),  # identical to h2\n",
    "    \"h4\": (0.80, 0.20),\n",
    "    \"h5\": (0.80, 0.20),\n",
    "}\n",
    "\n",
    "# 1c) Author CPTs given (mode, timing): (P(victim), P(offender))\n",
    "timing_author_pr = {\n",
    "    # h1\n",
    "    (\"h1\",\"t1\"): (0.98, 0.02),\n",
    "    (\"h1\",\"t2\"): (0.10, 0.90),\n",
    "    # h2\n",
    "    (\"h2\",\"t1\"): (0.98, 0.02),\n",
    "    (\"h2\",\"t2\"): (0.10, 0.90),\n",
    "    # h3 (copy of h2)\n",
    "    (\"h3\",\"t1\"): (0.98, 0.02),\n",
    "    (\"h3\",\"t2\"): (0.10, 0.90),\n",
    "    # h4\n",
    "    (\"h4\",\"t1\"): (0.90, 0.10),\n",
    "    (\"h4\",\"t2\"): (0.05, 0.95),\n",
    "    # h5\n",
    "    (\"h5\",\"t1\"): (0.98, 0.02),\n",
    "    (\"h5\",\"t2\"): (0.02, 0.98),\n",
    "}\n",
    "\n",
    "# 1d) Raw 10-bin blood patterns | (h,t,a)\n",
    "blood_pr_raw = {\n",
    "    # h1\n",
    "    (\"h1\",\"t1\",\"a1\"): [0.04,0.01,0.035,0.015,0.045,0.005,0.095,0.005,0.075,0.675],\n",
    "    (\"h1\",\"t1\",\"a2\"): [0.40,0.05,0.20, 0.10, 0.10, 0.05, 0.05, 0.03, 0.02, 0.00],\n",
    "    (\"h1\",\"t2\",\"a3\"): [0.30,0.10,0.20, 0.05, 0.15, 0.05, 0.10, 0.03, 0.02, 0.00],\n",
    "    (\"h1\",\"t2\",\"a4\"): [0.20,0.15,0.15, 0.10, 0.10, 0.10, 0.10, 0.05, 0.05, 0.00],\n",
    "\n",
    "    # h2\n",
    "    (\"h2\",\"t1\",\"a1\"): [0.05,0.03,0.04,0.02,0.06,0.02,0.08,0.01,0.05,0.64],\n",
    "    (\"h2\",\"t1\",\"a2\"): [0.35,0.05,0.25,0.10,0.10,0.05,0.05,0.02,0.03,0.00],\n",
    "    (\"h2\",\"t2\",\"a3\"): [0.25,0.10,0.15,0.10,0.15,0.05,0.10,0.05,0.05,0.00],\n",
    "    (\"h2\",\"t2\",\"a4\"): [0.15,0.15,0.10,0.10,0.10,0.10,0.15,0.10,0.05,0.00],\n",
    "\n",
    "    # h3 (copy of h2)\n",
    "    (\"h3\",\"t1\",\"a1\"): [0.05,0.03,0.04,0.02,0.06,0.02,0.08,0.01,0.05,0.64],\n",
    "    (\"h3\",\"t1\",\"a2\"): [0.35,0.05,0.25,0.10,0.10,0.05,0.05,0.02,0.03,0.00],\n",
    "    (\"h3\",\"t2\",\"a3\"): [0.25,0.10,0.15,0.10,0.15,0.05,0.10,0.05,0.05,0.00],\n",
    "    (\"h3\",\"t2\",\"a4\"): [0.15,0.15,0.10,0.10,0.10,0.10,0.15,0.10,0.05,0.00],\n",
    "\n",
    "    # h4\n",
    "    (\"h4\",\"t1\",\"a1\"): [0.03,0.01,0.02,0.01,0.04,0.01,0.04,0.01,0.03,0.80],\n",
    "    (\"h4\",\"t1\",\"a2\"): [0.25,0.05,0.20,0.10,0.10,0.05,0.05,0.02,0.03,0.15],\n",
    "    (\"h4\",\"t2\",\"a3\"): [0.10,0.10,0.10,0.10,0.10,0.10,0.10,0.10,0.10,0.10],\n",
    "    (\"h4\",\"t2\",\"a4\"): [0.10,0.15,0.10,0.10,0.10,0.10,0.10,0.10,0.05,0.10],\n",
    "\n",
    "    # h5\n",
    "    (\"h5\",\"t1\",\"a1\"): [0.02,0.005,0.015,0.005,0.03,0.005,0.08,0.001,0.04,0.799],\n",
    "    (\"h5\",\"t1\",\"a2\"): [0.20,0.05,0.15,0.10,0.10,0.05,0.05,0.03,0.02,0.25],\n",
    "    (\"h5\",\"t2\",\"a3\"): [0.25,0.10,0.20,0.10,0.15,0.05,0.10,0.02,0.03,0.00],\n",
    "    (\"h5\",\"t2\",\"a4\"): [0.15,0.15,0.10,0.10,0.10,0.10,0.15,0.10,0.05,0.00],\n",
    "}\n",
    "\n",
    "# 1e) Handwriting reliability offsets\n",
    "base_hw = 0.90\n",
    "mode_offset   = {\"h1\":+0.02,\"h2\":+0.01,\"h3\":+0.01,\"h4\":-0.02,\"h5\":-0.03}\n",
    "author_offset = {\"a1\":+0.01,\"a2\":-0.01,\"a3\":+0.01,\"a4\":-0.01}\n",
    "\n",
    "# 1f) Raw DNA accuracies | (h,t,a,e)\n",
    "dna_pr_raw = {\n",
    "    # h1\n",
    "    (\"h1\",\"t1\",\"a1\",\"e1\"): (0.995,0.005), (\"h1\",\"t1\",\"a1\",\"e3\"): (0.990,0.010),\n",
    "    (\"h1\",\"t1\",\"a1\",\"e5\"): (0.993,0.007), (\"h1\",\"t1\",\"a1\",\"e7\"): (0.992,0.008),\n",
    "    (\"h1\",\"t1\",\"a1\",\"e9\"): (0.994,0.006),\n",
    "    (\"h1\",\"t1\",\"a2\",\"e11\"): (0.980,0.020), (\"h1\",\"t1\",\"a2\",\"e13\"): (0.985,0.015),\n",
    "    (\"h1\",\"t1\",\"a2\",\"e15\"): (0.975,0.025), (\"h1\",\"t1\",\"a2\",\"e17\"): (0.982,0.018),\n",
    "    (\"h1\",\"t1\",\"a2\",\"e19\"): (0.988,0.012),\n",
    "    (\"h1\",\"t2\",\"a3\",\"e21\"): (0.960,0.040), (\"h1\",\"t2\",\"a3\",\"e23\"): (0.965,0.035),\n",
    "    (\"h1\",\"t2\",\"a3\",\"e25\"): (0.958,0.042), (\"h1\",\"t2\",\"a3\",\"e27\"): (0.962,0.038),\n",
    "    (\"h1\",\"t2\",\"a3\",\"e29\"): (0.967,0.033),\n",
    "    (\"h1\",\"t2\",\"a4\",\"e31\"): (0.940,0.060), (\"h1\",\"t2\",\"a4\",\"e33\"): (0.948,0.052),\n",
    "    (\"h1\",\"t2\",\"a4\",\"e35\"): (0.943,0.057), (\"h1\",\"t2\",\"a4\",\"e37\"): (0.950,0.050),\n",
    "    (\"h1\",\"t2\",\"a4\",\"e39\"): (0.945,0.055),\n",
    "\n",
    "    # h2\n",
    "    (\"h2\",\"t1\",\"a1\",\"e1\"): (0.990,0.010), (\"h2\",\"t1\",\"a1\",\"e3\"): (0.992,0.008),\n",
    "    (\"h2\",\"t1\",\"a1\",\"e5\"): (0.988,0.012), (\"h2\",\"t1\",\"a1\",\"e7\"): (0.991,0.009),\n",
    "    (\"h2\",\"t1\",\"a1\",\"e9\"): (0.989,0.011),\n",
    "    (\"h2\",\"t1\",\"a2\",\"e11\"): (0.970,0.030), (\"h2\",\"t1\",\"a2\",\"e13\"): (0.975,0.025),\n",
    "    (\"h2\",\"t1\",\"a2\",\"e15\"): (0.965,0.035), (\"h2\",\"t1\",\"a2\",\"e17\"): (0.972,0.028),\n",
    "    (\"h2\",\"t1\",\"a2\",\"e19\"): (0.968,0.032),\n",
    "    (\"h2\",\"t2\",\"a3\",\"e21\"): (0.950,0.050), (\"h2\",\"t2\",\"a3\",\"e23\"): (0.955,0.045),\n",
    "    (\"h2\",\"t2\",\"a3\",\"e25\"): (0.948,0.052), (\"h2\",\"t2\",\"a3\",\"e27\"): (0.952,0.048),\n",
    "    (\"h2\",\"t2\",\"a3\",\"e29\"): (0.957,0.043),\n",
    "    (\"h2\",\"t2\",\"a4\",\"e31\"): (0.930,0.070), (\"h2\",\"t2\",\"a4\",\"e33\"): (0.938,0.062),\n",
    "    (\"h2\",\"t2\",\"a4\",\"e35\"): (0.933,0.067), (\"h2\",\"t2\",\"a4\",\"e37\"): (0.940,0.060),\n",
    "    (\"h2\",\"t2\",\"a4\",\"e39\"): (0.935,0.065),\n",
    "\n",
    "    # h3 (copy of h2)\n",
    "    (\"h3\",\"t1\",\"a1\",\"e1\"): (0.990,0.010), (\"h3\",\"t1\",\"a1\",\"e3\"): (0.992,0.008),\n",
    "    (\"h3\",\"t1\",\"a1\",\"e5\"): (0.988,0.012), (\"h3\",\"t1\",\"a1\",\"e7\"): (0.991,0.009),\n",
    "    (\"h3\",\"t1\",\"a1\",\"e9\"): (0.989,0.011),\n",
    "    (\"h3\",\"t1\",\"a2\",\"e11\"): (0.970,0.030), (\"h3\",\"t1\",\"a2\",\"e13\"): (0.975,0.025),\n",
    "    (\"h3\",\"t1\",\"a2\",\"e15\"): (0.965,0.035), (\"h3\",\"t1\",\"a2\",\"e17\"): (0.972,0.028),\n",
    "    (\"h3\",\"t1\",\"a2\",\"e19\"): (0.968,0.032),\n",
    "    (\"h3\",\"t2\",\"a3\",\"e21\"): (0.950,0.050), (\"h3\",\"t2\",\"a3\",\"e23\"): (0.955,0.045),\n",
    "    (\"h3\",\"t2\",\"a3\",\"e25\"): (0.948,0.052), (\"h3\",\"t2\",\"a3\",\"e27\"): (0.952,0.048),\n",
    "    (\"h3\",\"t2\",\"a3\",\"e29\"): (0.957,0.043),\n",
    "    (\"h3\",\"t2\",\"a4\",\"e31\"): (0.930,0.070), (\"h3\",\"t2\",\"a4\",\"e33\"): (0.938,0.072),\n",
    "    (\"h3\",\"t2\",\"a4\",\"e35\"): (0.923,0.077), (\"h3\",\"t2\",\"a4\",\"e37\"): (0.930,0.070),\n",
    "    (\"h3\",\"t2\",\"a4\",\"e39\"): (0.925,0.075),\n",
    "\n",
    "    # h4\n",
    "    (\"h4\",\"t1\",\"a1\",\"e1\") : (0.980, 0.020),\n",
    "    (\"h4\",\"t1\",\"a1\",\"e3\") : (0.982, 0.018),\n",
    "    (\"h4\",\"t1\",\"a1\",\"e5\") : (0.978, 0.022),\n",
    "    (\"h4\",\"t1\",\"a1\",\"e7\") : (0.981, 0.019),\n",
    "    (\"h4\",\"t1\",\"a1\",\"e9\") : (0.979, 0.021),\n",
    "    (\"h4\",\"t1\",\"a2\",\"e11\"): (0.950, 0.050),\n",
    "    (\"h4\",\"t1\",\"a2\",\"e13\"): (0.955, 0.045),\n",
    "    (\"h4\",\"t1\",\"a2\",\"e15\"): (0.945, 0.055),\n",
    "    (\"h4\",\"t1\",\"a2\",\"e17\"): (0.952, 0.048),\n",
    "    (\"h4\",\"t1\",\"a2\",\"e19\"): (0.948, 0.052),\n",
    "    (\"h4\",\"t2\",\"a3\",\"e21\"): (0.940, 0.060),\n",
    "    (\"h4\",\"t2\",\"a3\",\"e23\"): (0.945, 0.055),\n",
    "    (\"h4\",\"t2\",\"a3\",\"e25\"): (0.938, 0.062),\n",
    "    (\"h4\",\"t2\",\"a3\",\"e27\"): (0.942, 0.058),\n",
    "    (\"h4\",\"t2\",\"a3\",\"e29\"): (0.947, 0.053),\n",
    "    (\"h4\",\"t2\",\"a4\",\"e31\"): (0.920, 0.080),\n",
    "    (\"h4\",\"t2\",\"a4\",\"e33\"): (0.928, 0.072),\n",
    "    (\"h4\",\"t2\",\"a4\",\"e35\"): (0.923, 0.077),\n",
    "    (\"h4\",\"t2\",\"a4\",\"e37\"): (0.930, 0.070),\n",
    "    (\"h4\",\"t2\",\"a4\",\"e39\"): (0.925, 0.075),\n",
    "\n",
    "    # h5\n",
    "    (\"h5\",\"t1\",\"a1\",\"e1\") : (0.975, 0.025),\n",
    "    (\"h5\",\"t1\",\"a1\",\"e3\") : (0.978, 0.022),\n",
    "    (\"h5\",\"t1\",\"a1\",\"e5\") : (0.973, 0.027),\n",
    "    (\"h5\",\"t1\",\"a1\",\"e7\") : (0.976, 0.024),\n",
    "    (\"h5\",\"t1\",\"a1\",\"e9\") : (0.974, 0.026),\n",
    "    (\"h5\",\"t1\",\"a2\",\"e11\"): (0.950, 0.050),\n",
    "    (\"h5\",\"t1\",\"a2\",\"e13\"): (0.955, 0.045),\n",
    "    (\"h5\",\"t1\",\"a2\",\"e15\"): (0.945, 0.055),\n",
    "    (\"h5\",\"t1\",\"a2\",\"e17\"): (0.952, 0.048),\n",
    "    (\"h5\",\"t1\",\"a2\",\"e19\"): (0.948, 0.052),\n",
    "    (\"h5\",\"t2\",\"a3\",\"e21\"): (0.940, 0.060),\n",
    "    (\"h5\",\"t2\",\"a3\",\"e23\"): (0.945, 0.055),\n",
    "    (\"h5\",\"t2\",\"a3\",\"e25\"): (0.938, 0.062),\n",
    "    (\"h5\",\"t2\",\"a3\",\"e27\"): (0.942, 0.058),\n",
    "    (\"h5\",\"t2\",\"a3\",\"e29\"): (0.947, 0.053),\n",
    "    (\"h5\",\"t2\",\"a4\",\"e31\"): (0.920, 0.080),\n",
    "    (\"h5\",\"t2\",\"a4\",\"e33\"): (0.928, 0.072),\n",
    "    (\"h5\",\"t2\",\"a4\",\"e35\"): (0.923, 0.077),\n",
    "    (\"h5\",\"t2\",\"a4\",\"e37\"): (0.930, 0.070),\n",
    "    (\"h5\",\"t2\",\"a4\",\"e39\"): (0.925, 0.075),\n",
    "}\n",
    "\n",
    "# After dna_pr_raw is constructed, swap for offender-authored a2/a4\n",
    "for (h, t, a, e), (pV, pO) in list(dna_pr_raw.items()):\n",
    "    if a in ('a2', 'a4'):     # offender wrote\n",
    "        dna_pr_raw[(h, t, a, e)] = (pO, pV)   # make DNA=O large when offender wrote\n",
    "\n",
    "# Timing splits when a letter is NOT found (L='no')\n",
    "mode_timing_pr_Lno = {\n",
    "    \"h1\": (0.85, 0.15),\n",
    "    \"h2\": (0.55, 0.45),\n",
    "    \"h3\": (0.55, 0.45),\n",
    "    \"h4\": (0.65, 0.35),\n",
    "    \"h5\": (0.80, 0.20),\n",
    "}\n",
    "# Timing splits when a letter IS found (L='yes')\n",
    "mode_timing_pr_Lyes = mode_timing_pr\n",
    "\n",
    "AUTHORS_BY_T = {'t1': ['a1','a2'], 't2': ['a3','a4']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f20aa020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise & build binary blood_yes_no_pr \n",
    "blood_pr = { key: normalise(raw) for key, raw in blood_pr_raw.items() }\n",
    "for dist10 in blood_pr.values():\n",
    "    assert abs(sum(dist10) - 1.0) < 1e-8\n",
    "\n",
    "blood_yes_no_pr = {}\n",
    "for (h, t, a), dist10 in blood_pr.items():\n",
    "    p_yes = sum(dist10[i] for i in range(0, 10, 2))\n",
    "    blood_yes_no_pr[(h, t, a)] = (p_yes, 1.0 - p_yes)\n",
    "\n",
    "# (g) Audit-node CPTs (keep AL simple; make AS depend on BP)\n",
    "alpha_S, beta_S = 0.80, 0.90\n",
    "alpha_F, beta_F = 0.85, 0.95\n",
    "alpha_E, beta_E = 0.90, 0.98\n",
    "\n",
    "mode_offset[\"h3\"] = mode_offset[\"h2\"]\n",
    "\n",
    "# Build handwriting_pr\n",
    "handwriting_pr = {}\n",
    "for (h, t, a) in blood_pr_raw:\n",
    "    p = base_hw + mode_offset[h] + author_offset[a]\n",
    "    p = min(max(p, HW_MIN), HW_MAX)\n",
    "    handwriting_pr[(h, t, a)] = (p, 1.0 - p)\n",
    "\n",
    "# normalised DNA\n",
    "dna_pr = {}\n",
    "for key, (p0, p1) in dna_pr_raw.items():\n",
    "    S = p0 + p1\n",
    "    dna_pr[key] = (p0 / S, p1 / S)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5083172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group-wise overrides \n",
    "override_mode_timing([\"h4\",\"h5\"], (0.80, 0.20))\n",
    "override_timing_author([\"h4\",\"h5\"], \"t1\", (0.90, 0.10))\n",
    "override_timing_author([\"h4\",\"h5\"], \"t2\", (0.05, 0.95))\n",
    "\n",
    "# Ensure h3 shares h2 CPTs (run once here)\n",
    "mode_timing_pr[\"h3\"] = mode_timing_pr[\"h2\"]\n",
    "for t in (\"t1\",\"t2\"):\n",
    "    timing_author_pr[(\"h3\", t)] = timing_author_pr[(\"h2\", t)]\n",
    "\n",
    "# Copy raw tables h2 -> h3 (if not already)\n",
    "for (h, t, a), raw in list(blood_pr_raw.items()):\n",
    "    if h == \"h2\":\n",
    "        blood_pr_raw[(\"h3\", t, a)] = raw.copy()\n",
    "\n",
    "for (h, t, a, e), pair in list(dna_pr_raw.items()):\n",
    "    if h == \"h2\":\n",
    "        dna_pr_raw[(\"h3\", t, a, e)] = pair\n",
    "\n",
    "# Rebuild blood_yes_no_pr after copies\n",
    "blood_pr = {k: normalise(v) for k, v in blood_pr_raw.items()}  # each sums to 1\n",
    "blood_yes_no_pr = {}\n",
    "for (h, t, a), dist10 in blood_pr.items():\n",
    "    p_yes = sum(dist10[i] for i in range(0, 10, 2))   # even bins = \"blood yes\"\n",
    "    blood_yes_no_pr[(h, t, a)] = (p_yes, 1.0 - p_yes)\n",
    "\n",
    "# 3) Handwriting per (h,t,a)\n",
    "HW_MIN, HW_MAX = 0.60, 0.95\n",
    "base_hw = 0.90\n",
    "mode_offset[\"h3\"] = mode_offset[\"h2\"]  # ensure same offset as h2\n",
    "\n",
    "handwriting_pr = {}\n",
    "for (h, t, a) in blood_pr_raw:\n",
    "    p = base_hw + mode_offset[h] + author_offset[a]\n",
    "    p = min(max(p, HW_MIN), HW_MAX)\n",
    "    handwriting_pr[(h, t, a)] = (p, 1.0 - p)\n",
    "\n",
    "# 4) Timing splits conditioned on L\n",
    "mode_timing_pr_Lyes = mode_timing_pr  # alias to be explicit\n",
    "\n",
    "# 5) Completeness checks\n",
    "missing_author = [(h, t) for h in H_STATES for t in ['t1','t2']\n",
    "                  if (h, t) not in timing_author_pr]\n",
    "assert not missing_author, f\"timing_author_pr missing: {missing_author}\"\n",
    "\n",
    "missing_blood = [\n",
    "    (h, t, a)\n",
    "    for h in H_STATES\n",
    "    for t in ['t1','t2']\n",
    "    for a in AUTHORS_BY_T[t]\n",
    "    if (h, t, a) not in blood_yes_no_pr\n",
    "]\n",
    "assert not missing_blood, f\"blood_yes_no_pr missing: {missing_blood}\"\n",
    "\n",
    "missing_hw = [\n",
    "    (h, t, a)\n",
    "    for h in H_STATES\n",
    "    for t in ['t1','t2']\n",
    "    for a in AUTHORS_BY_T[t]\n",
    "    if (h, t, a) not in handwriting_pr\n",
    "]\n",
    "assert not missing_hw, f\"handwriting_pr missing: {missing_hw}\"\n",
    "\n",
    "missing_Lsplits = [h for h in H_STATES\n",
    "                   if (h not in mode_timing_pr_Lyes) or (h not in mode_timing_pr_Lno)]\n",
    "assert not missing_Lsplits, f\"Timing splits missing for some h in Lyes/Lno: {missing_Lsplits}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c86f3603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Mappings between codes and BN variables/states\n",
    "H_MAP = {  # h-codes -> H states used in the notebook\n",
    "    \"h1\":\"O_delib\", \"h2\":\"O_self\", \"h3\":\"O_acc\", \"h4\":\"V_acc\", \"h5\":\"Suicide\"\n",
    "}\n",
    "T_MAP = {\"t1\":\"Before\", \"t2\":\"After\"}        # timing\n",
    "W_STATES = [\"Victim\",\"Offender\"]\n",
    "\n",
    "def a_code_for(W, t):\n",
    "    \"\"\"Map (writer, timing) -> 'a' code used in blood/DNA tables.\"\"\"\n",
    "    return {\"Victim\":{\"t1\":\"a1\",\"t2\":\"a3\"},\n",
    "            \"Offender\":{\"t1\":\"a2\",\"t2\":\"a4\"}}[W][t]\n",
    "\n",
    "# 2.2 Provide P(Lfound|H)\n",
    "LFOUND_YES = {\n",
    "    \"h1\":0.70, \"h2\":0.50, \"h3\":0.50, \"h4\":0.90, \"h5\":0.95\n",
    "}\n",
    "\n",
    "# 2.3 Helper to normalise a vector without epsilon smoothing\n",
    "def nz_normalise(v):\n",
    "    \"\"\"normalise a vector to sum to one; requires strictly positive sum.\"\"\"\n",
    "    s = sum(v)\n",
    "    assert s > 0, \"Zero-sum vector\"\n",
    "    return [x/s for x in v]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CPDs from edge tables\n",
    "\n",
    "def build_cpds_from_edge_tables_v2():\n",
    "    \"\"\"\n",
    "    Build path-aware CPDs that match a probability-tree/CEG view.\n",
    "    Returns dict of BN loader keys.\n",
    "    \"\"\"\n",
    "    H_ord = [\"h1\",\"h2\",\"h3\",\"h4\",\"h5\"]\n",
    "    T_ord = [\"t1\",\"t2\"]  # t1=Before, t2=After\n",
    "    W_ord = [\"Victim\",\"Offender\"]\n",
    "    Lf_ord = [\"No\",\"Yes\"]\n",
    "    Blood_states = [\"No\",\"Yes\"]\n",
    "    BP_states = [\"Fingerprint-like\",\"Spatter\"]\n",
    "    DNA_states = [\"V\",\"O\"]\n",
    "\n",
    "    # 0) Prior on H\n",
    "    prior_H = [mode_pr[h] for h in H_ord]\n",
    "\n",
    "    # 1) P(Lfound | H)\n",
    "    Lfound_table = [\n",
    "        [1 - LFOUND_YES[h] for h in H_ord],   # No\n",
    "        [    LFOUND_YES[h] for h in H_ord],   # Yes\n",
    "    ]\n",
    "\n",
    "    # 2) P(Ltime | H, Lfound)\n",
    "    cols_before, cols_after = [], []\n",
    "    for h in H_ord:\n",
    "        for lf in Lf_ord:\n",
    "            b, a = (mode_timing_pr_Lno[h] if lf == \"No\" else mode_timing_pr_Lyes[h])\n",
    "            cols_before.append(b)\n",
    "            cols_after.append(a)\n",
    "    Ltime_table = [cols_before, cols_after]  # rows Before, After\n",
    "\n",
    "    # 3) P(W | H, Ltime)\n",
    "    WV, WO = [], []\n",
    "    for h in H_ord:\n",
    "        for t in T_ord:\n",
    "            pV, pO = timing_author_pr[(h, t)]\n",
    "            WV.append(pV); WO.append(pO)\n",
    "    W_table = [WV, WO]\n",
    "\n",
    "    # Local helper\n",
    "    def a_code_for(W, t):\n",
    "        return {\"Victim\":{\"t1\":\"a1\",\"t2\":\"a3\"},\n",
    "                \"Offender\":{\"t1\":\"a2\",\"t2\":\"a4\"}}[W][t]\n",
    "\n",
    "    # 4) P(Blood | H, Ltime, W)\n",
    "    blood_rows_no, blood_rows_yes = [], []\n",
    "    for h in H_ord:\n",
    "        for t in T_ord:\n",
    "            for W in W_ord:\n",
    "                a = a_code_for(W, t)\n",
    "                p_yes, p_no = blood_yes_no_pr[(h, t, a)]\n",
    "                blood_rows_no.append(p_no)\n",
    "                blood_rows_yes.append(p_yes)\n",
    "    Blood_table = [blood_rows_no, blood_rows_yes]\n",
    "\n",
    "    # 5) P(BP | Blood)\n",
    "    PHI_BP = 0.60\n",
    "    BP_table = [\n",
    "        [1.0,        PHI_BP],      # Fingerprint-like\n",
    "        [0.0, 1.0 - PHI_BP],       # Spatter\n",
    "    ]\n",
    "\n",
    "    # 6) P(DNA | H, Ltime, W) by averaging across e\n",
    "    DNA_V, DNA_O = [], []\n",
    "    e_groups = {}\n",
    "    for (h, t, a, e), (p0, p1) in dna_pr_raw.items():\n",
    "        e_groups.setdefault((h, t, a), []).append((p0, p1))\n",
    "    for h in H_ord:\n",
    "        for t in T_ord:\n",
    "            for W in W_ord:\n",
    "                a = a_code_for(W, t)\n",
    "                pairs = e_groups.get((h, t, a), [])\n",
    "                if not pairs:\n",
    "                    pO = 0.5\n",
    "                else:\n",
    "                    pO = float(np.mean([p1/(p0+p1) for (p0, p1) in pairs]))\n",
    "                pV = 1.0 - pO\n",
    "                DNA_V.append(pV); DNA_O.append(pO)\n",
    "    DNA_table = [DNA_V, DNA_O]\n",
    "\n",
    "    # 7) P(AL | DNA)\n",
    "    AL_PASS = 0.90\n",
    "    AL_table = [\n",
    "        [1 - AL_PASS, 1 - AL_PASS],  # Fail\n",
    "        [    AL_PASS,     AL_PASS],  # Pass\n",
    "    ]\n",
    "\n",
    "    # 8) P(AS | BP)\n",
    "    AS_PASS = 0.90\n",
    "    AS_table = [\n",
    "        [1 - AS_PASS, 1 - AS_PASS],  # Fail\n",
    "        [    AS_PASS,     AS_PASS],  # Pass\n",
    "    ]\n",
    "\n",
    "    # 9) P(Hand | W) (marginalised reliability)\n",
    "    def p_hand_V_given_W(W):\n",
    "        num = den = 0.0\n",
    "        for h in H_ord:\n",
    "            pH = mode_pr[h]\n",
    "            for lf in Lf_ord:\n",
    "                pLf = (1 - LFOUND_YES[h]) if lf == \"No\" else LFOUND_YES[h]\n",
    "                b, a = (mode_timing_pr_Lno[h] if lf == \"No\" else mode_timing_pr_Lyes[h])\n",
    "                for t, pT in zip(T_ord, [b, a]):\n",
    "                    pV, pO = timing_author_pr[(h, t)]\n",
    "                    w_W = pV if W == \"Victim\" else pO\n",
    "                    wgt = pH * pLf * pT * w_W\n",
    "                    if wgt == 0.0:\n",
    "                        continue\n",
    "                    a_code = a_code_for(W, t)\n",
    "                    pV0, pO0 = handwriting_pr[(h, t, a_code)]\n",
    "                    num += wgt * pV0\n",
    "                    den += wgt\n",
    "        return num / den if den > 0 else 0.5\n",
    "\n",
    "    Hand_V, Hand_O = [], []\n",
    "    for W in W_ord:\n",
    "        pV = p_hand_V_given_W(W)\n",
    "        Hand_V.append(pV); Hand_O.append(1.0 - pV)\n",
    "    Hand_table = [Hand_V, Hand_O]\n",
    "\n",
    "    return {\n",
    "        'H_prior'       : prior_H,\n",
    "        'Lfound|H'      : Lfound_table,\n",
    "        'Ltime|H,Lfound': Ltime_table,\n",
    "        'W|H,Ltime'     : W_table,\n",
    "        'Blood|H,Ltime,W': Blood_table,\n",
    "        'BP|Blood'      : BP_table,\n",
    "        'DNA|H,Ltime,W' : DNA_table,\n",
    "        'AL|DNA'        : AL_table,\n",
    "        'AS|BP'         : AS_table,\n",
    "        'Hand|W'        : Hand_table,\n",
    "    }\n",
    "\n",
    "# Build once\n",
    "EDGE_CPDS = build_cpds_from_edge_tables_v2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For consistent results\n",
    "random.seed(7)\n",
    "np.random.seed(7)\n",
    "\n",
    "# Variables, states, display names, orders\n",
    "\n",
    "\n",
    "VAR_STATES = {\n",
    "    'H'     : ['O_delib','O_self','O_acc','V_acc','Suicide'],\n",
    "    'Lfound': ['No','Yes'],\n",
    "    'Ltime' : ['Before','After'],\n",
    "    'O'     : ['Absent','Present'],\n",
    "    'W'     : ['Victim','Offender'],\n",
    "    'Blood' : ['No','Yes'],\n",
    "    'BP'    : ['Fingerprint-like','Spatter'],\n",
    "    'DNA'   : ['V','O'],\n",
    "    'AL'    : ['Fail','Pass'],\n",
    "    'AS'    : ['Fail','Pass'],\n",
    "    'Hand'  : ['Victim','Offender'],\n",
    "}\n",
    "\n",
    "STATE_LABEL = {\n",
    "    'H': {'O_delib':'Offender deliberate','O_self':'Offender self-defense',\n",
    "          'O_acc':'Offender accidental','V_acc':'Victim accidental','Suicide':'Suicide'},\n",
    "    'Lfound': {'No':'Letter not found','Yes':'Letter found'},\n",
    "    'Ltime': {'Before':'Letter before death','After':'Letter after death'},\n",
    "    'O': {'Absent':'Offender absent','Present':'Offender present'},\n",
    "    'W': {'Victim':'Victim wrote','Offender':'Offender wrote'},\n",
    "    'Blood': {'No':'NBlood','Yes':'Blood'},\n",
    "    'BP': {'Fingerprint-like':'Blood pattern: Fingerprint-like','Spatter':'Blood pattern: Spatter'},\n",
    "    'DNA': {'V':'DNA: Victim','O':'DNA: Offender'},\n",
    "    'AL': {'Fail':'Audit lab: Fail','Pass':'Audit lab: Pass'},\n",
    "    'AS': {'Fail':'Audit scene: Fail','Pass':'Audit scene: Pass'},\n",
    "    'Hand': {'Victim':'Handwriting: Victim','Offender':'Handwriting: Offender'},\n",
    "}\n",
    "\n",
    "SHORT_LABEL = {\n",
    "  'H': {\n",
    "    'O_delib':'O delib','O_self':'O self-def','O accident':'O accident',\n",
    "    'V_acc':'V accident','Suicide':'Suicide'\n",
    "  },\n",
    "  'Lfound': {'No':'L? No','Yes':'L? Yes'},\n",
    "  'Ltime': {'Before':'t: Before','After':'t: After'},\n",
    "  'O': {'Absent':'O: Absent','Present':'O: Present'},\n",
    "  'W': {'Victim':'W: Victim','Offender':'W: Offender'},\n",
    "  'Blood': {'No':'Blood: No','Yes':'Blood: Yes'},\n",
    "  'BP': {'Fingerprint-like':'BP: Fingerprint','Spatter':'BP: Spat'},\n",
    "  'DNA': {'V':'DNA: V','O':'DNA: O'},\n",
    "  'AL': {'Fail':'AL: Fail','Pass':'AL: Pass'},\n",
    "  'AS': {'Fail':'AS: Fail','Pass':'AS: Pass'},\n",
    "  'Hand': {'Victim':'Hand: V','Offender':'Hand: O'},\n",
    "}\n",
    "\n",
    "# Event-tree orders (display only)\n",
    "ORDER_HYPOTHESIS = ('H','Lfound','Ltime','O','W','Blood','BP','AS','DNA','AL','Hand')\n",
    "ORDER_CHRONO  = ('Lfound','Ltime','O','W','H','Blood','BP','AS','DNA','AL','Hand')\n",
    "\n",
    "AUDIT_VARS = {'AL','AS'}\n",
    "SINK_VAR   = 'Hand'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfa6e09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build BN model (path-aware CPDs)\n",
    "\n",
    "def make_model_once(edge_cpds=EDGE_CPDS):\n",
    "    \"\"\"Build a BN that reflects the path-aware CPDs. Used for VE queries.\"\"\"\n",
    "    model = DiscreteBayesianNetwork([\n",
    "        # letter + timing\n",
    "        ('H','Lfound'),\n",
    "        ('H','Ltime'), ('Lfound','Ltime'),\n",
    "\n",
    "        # presence and authorship\n",
    "        ('H','O'), ('Ltime','O'),\n",
    "        ('O','W'), ('H','W'), ('Ltime','W'),\n",
    "\n",
    "        # scene + lab\n",
    "        ('H','Blood'), ('Ltime','Blood'), ('W','Blood'),\n",
    "        ('Blood','BP'),\n",
    "        ('BP','AS'),\n",
    "        ('O','DNA'), ('W','DNA'),\n",
    "        ('DNA','AL'),\n",
    "        ('W','Hand'),\n",
    "    ])\n",
    "\n",
    "    def _to_table(var, table, evidence):\n",
    "        vcard = len(VAR_STATES[var])\n",
    "        ecards = [len(VAR_STATES[e]) for e in (evidence or [])]\n",
    "        prod_e = int(np.prod(ecards)) if ecards else 1\n",
    "        arr = np.asarray(table, dtype=float)\n",
    "        if arr.ndim == 1:\n",
    "            arr = arr.reshape((vcard, 1))\n",
    "        else:\n",
    "            if arr.shape == (1, vcard):\n",
    "                arr = arr.T\n",
    "            if arr.size == vcard * prod_e and arr.shape != (vcard, prod_e):\n",
    "                arr = arr.reshape((vcard, prod_e), order='C')\n",
    "        assert arr.shape == (vcard, prod_e), f\"{var} has shape {arr.shape}, expected {(vcard, prod_e)}\"\n",
    "        return arr.tolist(), ecards\n",
    "\n",
    "    def CPD(var, key, evidence=None):\n",
    "        evidence = evidence or []\n",
    "        table, ecards = _to_table(var, edge_cpds[key], evidence)\n",
    "        return TabularCPD(\n",
    "            variable=var,\n",
    "            variable_card=len(VAR_STATES[var]),\n",
    "            values=table,\n",
    "            evidence=evidence,\n",
    "            evidence_card=ecards,\n",
    "            state_names={var: VAR_STATES[var], **{e: VAR_STATES[e] for e in evidence}},\n",
    "        )\n",
    "\n",
    "    # Pull original per-(H,Ltime) P(W=Offender) and calibrate O, W\n",
    "    inv_H = {v: k for k, v in H_MAP.items()}\n",
    "    inv_T = {'Before': 't1', 'After': 't2'}\n",
    "\n",
    "    p_old_off_cols = [\n",
    "        timing_author_pr[(inv_H[h_lab], inv_T[t_lab])][1]\n",
    "        for h_lab in VAR_STATES['H']\n",
    "        for t_lab in VAR_STATES['Ltime']\n",
    "    ]\n",
    "\n",
    "    eps_w = 1e-6\n",
    "    theta = min(1-1e-6, max(p_old_off_cols))\n",
    "\n",
    "    def clip(x, lo=1e-6, hi=1-1e-6):\n",
    "        return max(lo, min(hi, x))\n",
    "\n",
    "    omega_cols = [clip(p_off / theta) for p_off in p_old_off_cols]\n",
    "\n",
    "    O_table = [\n",
    "        [1.0 - om for om in omega_cols],\n",
    "        [      om for om in omega_cols],\n",
    "    ]\n",
    "\n",
    "    # Build W table interleaving O=Absent, O=Present per (H,Ltime)\n",
    "    W_cols_V_abs, W_cols_O_abs, W_cols_V_pre, W_cols_O_pre = [], [], [], []\n",
    "    for _ in omega_cols:\n",
    "        W_cols_V_abs.append(1.0 - eps_w); W_cols_O_abs.append(eps_w)\n",
    "        W_cols_V_pre.append(1.0 - theta); W_cols_O_pre.append(theta)\n",
    "\n",
    "    W_table = [\n",
    "        [v for i in range(len(omega_cols)) for v in (W_cols_V_abs[i], W_cols_V_pre[i])],\n",
    "        [o for i in range(len(omega_cols)) for o in (W_cols_O_abs[i], W_cols_O_pre[i])],\n",
    "    ]\n",
    "\n",
    "    cpd_O = TabularCPD(\n",
    "        variable='O', variable_card=2, values=O_table,\n",
    "        evidence=['H','Ltime'], evidence_card=[5,2],\n",
    "        state_names={'O':VAR_STATES['O'], 'H':VAR_STATES['H'], 'Ltime':VAR_STATES['Ltime']}\n",
    "    )\n",
    "    cpd_W = TabularCPD(\n",
    "        variable='W', variable_card=2, values=W_table,\n",
    "        evidence=['H','Ltime','O'], evidence_card=[5,2,2],\n",
    "        state_names={'W':VAR_STATES['W'], 'H':VAR_STATES['H'], 'Ltime':VAR_STATES['Ltime'], 'O':VAR_STATES['O']}\n",
    "    )\n",
    "\n",
    "    vals = [\n",
    "        [0.98, 0.20, 0.85, 0.10],  # P(DNA='V' | O,W)\n",
    "        [0.02, 0.80, 0.15, 0.90],  # P(DNA='O' | O,W)\n",
    "    ]\n",
    "    cpd_DNA = TabularCPD(\n",
    "        variable='DNA', variable_card=2, values=vals,\n",
    "        evidence=['O','W'], evidence_card=[2,2],\n",
    "        state_names={'DNA':VAR_STATES['DNA'], 'O':VAR_STATES['O'], 'W':VAR_STATES['W']}\n",
    "    )\n",
    "\n",
    "    cpd_H       = CPD('H',       'H_prior')\n",
    "    cpd_Lfound  = CPD('Lfound',  'Lfound|H',        evidence=['H'])\n",
    "    cpd_Ltime   = CPD('Ltime',   'Ltime|H,Lfound',  evidence=['H','Lfound'])\n",
    "    cpd_Blood   = CPD('Blood',   'Blood|H,Ltime,W', evidence=['H','Ltime','W'])\n",
    "    cpd_BP      = CPD('BP',      'BP|Blood',        evidence=['Blood'])\n",
    "    cpd_AL      = CPD('AL',      'AL|DNA',          evidence=['DNA'])\n",
    "    cpd_AS      = CPD('AS',      'AS|BP',           evidence=['BP'])\n",
    "    cpd_Hand    = CPD('Hand',    'Hand|W',          evidence=['W'])\n",
    "\n",
    "    model.add_cpds(cpd_H, cpd_Lfound, cpd_Ltime, cpd_O, cpd_W,\n",
    "                   cpd_Blood, cpd_BP, cpd_DNA, cpd_AL, cpd_AS, cpd_Hand)\n",
    "    assert model.check_model()\n",
    "    return model\n",
    "\n",
    "# Chronology overlays (joint-preserving reparameterisation)\n",
    "\n",
    "def _nz_norm(a, axis=-1):\n",
    "    \"\"\"normalise numpy array along axis with safe zero handling.\"\"\"\n",
    "    s = a.sum(axis=axis, keepdims=True)\n",
    "    s[s == 0] = 1.0\n",
    "    return a / s\n",
    "\n",
    "def compute_chrono_overlays_from_model(model):\n",
    "    \"\"\"Compute P(Lfound), P(Ltime|Lfound), P(O|Ltime,Lfound), P(W|O,Ltime,Lfound),\n",
    "    and P(H|Lfound,Ltime,O,W) directly from the BN CPDs.\n",
    "    \"\"\"\n",
    "    H = len(VAR_STATES['H'])\n",
    "    Lf = len(VAR_STATES['Lfound'])\n",
    "    Lt = len(VAR_STATES['Ltime'])\n",
    "    O  = len(VAR_STATES['O'])\n",
    "    W  = len(VAR_STATES['W'])\n",
    "\n",
    "    P_H = np.array(model.get_cpds('H').get_values(), dtype=float).reshape(H)\n",
    "    P_Lf_given_H = np.array(model.get_cpds('Lfound').get_values(), dtype=float)\n",
    "    P_Lt_given_HLf = np.array(model.get_cpds('Ltime').get_values(), dtype=float)\n",
    "    P_O_given_HLt  = np.array(model.get_cpds('O').get_values(), dtype=float)\n",
    "    P_W_given_HLtO = np.array(model.get_cpds('W').get_values(), dtype=float)\n",
    "\n",
    "    P_Lf_given_H = P_Lf_given_H.reshape(Lf, H)\n",
    "    P_Lt_given_HLf = P_Lt_given_HLf.reshape(Lt, H, Lf)\n",
    "    P_O_given_HLt  = P_O_given_HLt.reshape(O,  H, Lt)\n",
    "    P_W_given_HLtO = P_W_given_HLtO.reshape(W,  H, Lt, O)\n",
    "\n",
    "    P_Lf = (P_Lf_given_H * P_H[None, :]).sum(axis=1)\n",
    "\n",
    "    num_H_given_Lf = P_Lf_given_H * P_H[None, :]\n",
    "    P_H_given_Lf   = _nz_norm(num_H_given_Lf, axis=1)\n",
    "\n",
    "    P_Lt_given_Lf = np.einsum('lh, thl -> lt', P_H_given_Lf, P_Lt_given_HLf)\n",
    "\n",
    "    num_H_given_LfLt = np.einsum('h, lh, thl -> lth', P_H, P_Lf_given_H, P_Lt_given_HLf)\n",
    "    P_H_given_LfLt   = _nz_norm(num_H_given_LfLt, axis=2)\n",
    "\n",
    "    P_O_given_LtLf = np.einsum('oht, lth -> lto', P_O_given_HLt, P_H_given_LfLt)\n",
    "    P_O_given_LtLf = _nz_norm(P_O_given_LtLf, axis=2)\n",
    "\n",
    "    num_H_given_LfLtO = np.einsum('lth,oht->ltoh', P_H_given_LfLt, P_O_given_HLt)\n",
    "    P_H_given_LfLtO   = _nz_norm(num_H_given_LfLtO, axis=3)\n",
    "\n",
    "    P_W_given_OLtLf = np.einsum('whlo,ltoh->ltow', P_W_given_HLtO, P_H_given_LfLtO)\n",
    "    P_W_given_OLtLf = _nz_norm(P_W_given_OLtLf, axis=3)\n",
    "\n",
    "    num_H_given_LfLtOW = np.einsum('ltoh,whlo->ltowh', P_H_given_LfLtO, P_W_given_HLtO)\n",
    "    P_H_given_LfLtOW   = _nz_norm(num_H_given_LfLtOW, axis=4)\n",
    "\n",
    "    return dict(\n",
    "        P_Lfound                 = P_Lf,\n",
    "        P_Ltime_given_Lfound     = P_Lt_given_Lf,\n",
    "        P_O_given_Ltime_Lfound   = P_O_given_LtLf,\n",
    "        P_W_given_O_Ltime_Lfound = P_W_given_OLtLf,\n",
    "        P_H_given_LfLtOW         = P_H_given_LfLtOW,\n",
    "    )\n",
    "\n",
    "def make_chrono_overrides(overlays):\n",
    "    \"\"\"Build callables that return probability vectors for overrides.\"\"\"\n",
    "    def _Lfound_vec(hist):\n",
    "        return overlays['P_Lfound']\n",
    "    def _Ltime_vec(hist):\n",
    "        lf = _hist_get_state(hist, 'Lfound'); i = _idx('Lfound', lf)\n",
    "        return overlays['P_Ltime_given_Lfound'][i, :]\n",
    "    def _O_vec(hist):\n",
    "        lf = _hist_get_state(hist, 'Lfound'); lt = _hist_get_state(hist, 'Ltime')\n",
    "        i = _idx('Lfound', lf); j = _idx('Ltime', lt)\n",
    "        return overlays['P_O_given_Ltime_Lfound'][i, j, :]\n",
    "    def _W_vec(hist):\n",
    "        lf = _hist_get_state(hist, 'Lfound'); lt = _hist_get_state(hist, 'Ltime'); o = _hist_get_state(hist, 'O')\n",
    "        i = _idx('Lfound', lf); j = _idx('Ltime', lt); k = _idx('O', o)\n",
    "        return overlays['P_W_given_O_Ltime_Lfound'][i, j, k, :]\n",
    "    def _H_vec(hist):\n",
    "        lf = _hist_get_state(hist, 'Lfound'); lt = _hist_get_state(hist, 'Ltime')\n",
    "        o  = _hist_get_state(hist, 'O');      w  = _hist_get_state(hist, 'W')\n",
    "        i = _idx('Lfound', lf); j = _idx('Ltime', lt); k = _idx('O', o); m = _idx('W', w)\n",
    "        return overlays['P_H_given_LfLtOW'][i, j, k, m, :]\n",
    "\n",
    "    return {'Lfound': _Lfound_vec, 'Ltime': _Ltime_vec, 'O': _O_vec, 'W': _W_vec, 'H': _H_vec}\n",
    "\n",
    "# Build model, VE, and overlays\n",
    "MODEL = make_model_once()\n",
    "VE    = VariableElimination(MODEL)\n",
    "\n",
    "_CHRONO_OVERLAYS = compute_chrono_overlays_from_model(MODEL)\n",
    "CHRONO_OVERRIDES = make_chrono_overrides(_CHRONO_OVERLAYS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1aaeaf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canonical labels & BN query \n",
    "\n",
    "def canonical_label(var, state):\n",
    "    \"\"\"Format Var:State labels used on edges and histories.\"\"\"\n",
    "    return f\"{var}:{state}\"\n",
    "\n",
    "def parse_label(label):\n",
    "    \"\"\"Split a canonical label into (var, state).\"\"\"\n",
    "    var, state = label.split(':', 1)\n",
    "    return var, state\n",
    "\n",
    "def history_to_evidence(history):\n",
    "    \"\"\"Convert a tuple/list of 'Var:State' labels to a pgmpy evidence dict.\"\"\"\n",
    "    hyp = {}\n",
    "    for lab in history:\n",
    "        v, s = parse_label(lab)\n",
    "        hyp[v] = s\n",
    "    return hyp\n",
    "\n",
    "def get_prob_bn(model, history, query_var, query_state):\n",
    "    \"\"\"P(query_var=query_state | history) via VE. If already fixed, return 0/1.\"\"\"\n",
    "    evidence = history_to_evidence(history)\n",
    "    if query_var in evidence:\n",
    "        return 1.0 if evidence[query_var] == query_state else 0.0\n",
    "    q = VE.query([query_var], evidence=evidence, show_progress=False)\n",
    "    states = model.get_cpds(query_var).state_names[query_var]\n",
    "    idx = states.index(query_state)\n",
    "    return float(q.values[idx])\n",
    "\n",
    "def get_prob_mixed(model, history, query_var, query_state, overrides=None):\n",
    "    \"\"\"Use overlay vector for query_var when provided, otherwise fall back to BN.\"\"\"\n",
    "    if overrides and query_var in overrides:\n",
    "        vec = np.asarray(overrides[query_var](history), dtype=float)\n",
    "        s = float(vec.sum())\n",
    "        if s <= 0:\n",
    "            return get_prob_bn(model, history, query_var, query_state)\n",
    "        vec = vec / s\n",
    "        idx = VAR_STATES[query_var].index(query_state)\n",
    "        return float(vec[idx])\n",
    "    return get_prob_bn(model, history, query_var, query_state)\n",
    "\n",
    "# Helpers for overrides\n",
    "def _hist_get_state(history, var):\n",
    "    for lab in history:\n",
    "        v, st = parse_label(lab)\n",
    "        if v == var:\n",
    "            return st\n",
    "    return None\n",
    "\n",
    "def _idx(var, st):\n",
    "    return VAR_STATES[var].index(st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55c48d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event-tree builder + deterministic contraction\n",
    "\n",
    "def build_event_tree(model, order, var_states, root_history=(), overrides=None):\n",
    "    \"\"\"Construct the event tree following a variable order, optionally using overlays.\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    root = root_history\n",
    "    G.add_node(root, history=root)\n",
    "    frontier = [root]\n",
    "\n",
    "    for var in order:\n",
    "        new_frontier = []\n",
    "        for u in frontier:\n",
    "            hist = list(G.nodes[u]['history'])\n",
    "            hist_vars = {parse_label(l)[0] for l in hist if ':' in l}\n",
    "            if var in hist_vars:\n",
    "                new_frontier.append(u)\n",
    "                continue\n",
    "            states = var_states[var]\n",
    "            probs  = [get_prob_mixed(model, hist, var, s, overrides=overrides) for s in states]\n",
    "            s = sum(probs); assert s > 0\n",
    "            probs = [p/s for p in probs]\n",
    "            labels = [canonical_label(var, s) for s in states]\n",
    "            assert len(labels) == len(set(labels))\n",
    "            for state, p, lab in zip(states, probs, labels):\n",
    "                if p <= 0.0:\n",
    "                    continue\n",
    "                v = tuple(hist + [lab])\n",
    "                G.add_node(v, history=v)\n",
    "                G.add_edge(u, v, prob=float(p), label=lab, var=var, state=state)\n",
    "                new_frontier.append(v)\n",
    "        frontier = new_frontier\n",
    "\n",
    "    contract_prob_one_edges(G)\n",
    "    return G\n",
    "\n",
    "def contract_prob_one_edges(G, tol=1e-12):\n",
    "    \"\"\"Contract edges with probability approximately 1 to avoid trivial nodes.\"\"\"\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        for u in list(G.nodes()):\n",
    "            succ = list(G.successors(u))\n",
    "            if not succ:\n",
    "                continue\n",
    "            if len(succ) == 1:\n",
    "                v = succ[0]\n",
    "                p = G[u][v]['prob']\n",
    "                if abs(p - 1.0) < tol:\n",
    "                    preds = list(G.predecessors(u))\n",
    "                    if preds:\n",
    "                        for a in preds:\n",
    "                            pa = G[a][u]['prob'] * p\n",
    "                            lab = G[a][u]['label']\n",
    "                            if G.has_edge(a, v):\n",
    "                                G[a][v]['prob'] += pa\n",
    "                            else:\n",
    "                                G.add_edge(a, v, prob=pa, label=lab)\n",
    "                            G.remove_edge(a, u)\n",
    "                    if u in G:\n",
    "                        G.remove_node(u)\n",
    "                    changed = True\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc8ceeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage & position (CEG) via partition refinement\n",
    "\n",
    "def compute_positions(G, prob_round=12, depth_lock=True):\n",
    "    \"\"\"Return maps: node->stage_id and node->position_id using refinement.\"\"\"\n",
    "    roots = [n for n in G.nodes() if G.in_degree(n) == 0]\n",
    "    assert len(roots) == 1, \"Graph must have a single root\"\n",
    "    root = roots[0]\n",
    "    depth = {root: 0}\n",
    "    for u in nx.topological_sort(G):\n",
    "        for v in G.successors(u):\n",
    "            depth[v] = depth[u] + 1\n",
    "\n",
    "    def stage_sig(u):\n",
    "        items = []\n",
    "        for v in G.successors(u):\n",
    "            lab = G[u][v]['label']\n",
    "            p   = round(float(G[u][v]['prob']), prob_round)\n",
    "            items.append((lab, p))\n",
    "        sig = tuple(sorted(items))\n",
    "        return (depth[u], sig) if depth_lock else sig\n",
    "\n",
    "    stage = {u: stage_sig(u) for u in G.nodes()}\n",
    "    pos   = {u: stage[u] for u in G.nodes()}\n",
    "\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        new_pos = {}\n",
    "        for u in G.nodes():\n",
    "            items = []\n",
    "            for v in G.successors(u):\n",
    "                lab = G[u][v]['label']\n",
    "                p   = round(float(G[u][v]['prob']), prob_round)\n",
    "                items.append((lab, p, pos.get(v, ('LEAF',))))\n",
    "            sig = (depth[u], tuple(sorted(items))) if depth_lock else tuple(sorted(items))\n",
    "            new_pos[u] = sig\n",
    "        if any(new_pos[u] != pos[u] for u in G.nodes()):\n",
    "            pos = new_pos\n",
    "            changed = True\n",
    "\n",
    "    def compress_to_ids(mapping):\n",
    "        uniq, ids = {}, {}\n",
    "        k = 0\n",
    "        for u, sig in mapping.items():\n",
    "            if sig not in uniq:\n",
    "                uniq[sig] = k; k += 1\n",
    "            ids[u] = uniq[sig]\n",
    "        return ids\n",
    "\n",
    "    return compress_to_ids(stage), compress_to_ids(pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "943e6ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audit elimination (handles chains) + renormalise\n",
    "\n",
    "def is_audit_node(node):\n",
    "    \"\"\"True if the last label in the node history belongs to an audit variable.\"\"\"\n",
    "    if not node:\n",
    "        return False\n",
    "    var, _ = parse_label(node[-1])\n",
    "    return var in AUDIT_VARS\n",
    "\n",
    "def strip_audits_from_history(history):\n",
    "    \"\"\"Return a tuple of labels with all AUDIT_VARS removed.\"\"\"\n",
    "    return tuple(lab for lab in history if parse_label(lab)[0] not in AUDIT_VARS)\n",
    "\n",
    "def eliminate_audits(G):\n",
    "    \"\"\"Remove all audit vertices by bridging over audit-only chains.\"\"\"\n",
    "    H = nx.DiGraph()\n",
    "\n",
    "    def ensure_node(hist_clean):\n",
    "        if hist_clean not in H:\n",
    "            H.add_node(hist_clean, history=hist_clean)\n",
    "\n",
    "    def first_non_audit_children(x, p_in):\n",
    "        stack = [(x, p_in)]\n",
    "        while stack:\n",
    "            u, pu = stack.pop()\n",
    "            for v in G.successors(u):\n",
    "                pv = pu * float(G[u][v]['prob'])\n",
    "                if is_audit_node(v):\n",
    "                    stack.append((v, pv))\n",
    "                else:\n",
    "                    yield v, pv\n",
    "\n",
    "    root = [n for n in G.nodes() if G.in_degree(n) == 0][0]\n",
    "    q = deque([root])\n",
    "    seen = set([root])\n",
    "\n",
    "    while q:\n",
    "        u_orig = q.popleft()\n",
    "        u_clean = strip_audits_from_history(u_orig)\n",
    "        ensure_node(u_clean)\n",
    "\n",
    "        for v in G.successors(u_orig):\n",
    "            p0 = float(G[u_orig][v]['prob'])\n",
    "            if is_audit_node(v):\n",
    "                for w, pw in first_non_audit_children(v, p0):\n",
    "                    lab = G[v][w]['label']\n",
    "                    child_clean = u_clean + (lab,)\n",
    "                    ensure_node(child_clean)\n",
    "                    var, state = parse_label(lab)\n",
    "                    if H.has_edge(u_clean, child_clean):\n",
    "                        H[u_clean][child_clean]['prob'] += pw\n",
    "                    else:\n",
    "                        H.add_edge(u_clean, child_clean, prob=pw,\n",
    "                                   label=lab, var=var, state=state)\n",
    "                    if w not in seen:\n",
    "                        seen.add(w); q.append(w)\n",
    "            else:\n",
    "                lab = G[u_orig][v]['label']\n",
    "                child_clean = u_clean + (lab,)\n",
    "                ensure_node(child_clean)\n",
    "                var, state = parse_label(lab)\n",
    "                if H.has_edge(u_clean, child_clean):\n",
    "                    H[u_clean][child_clean]['prob'] += p0\n",
    "                else:\n",
    "                    H.add_edge(u_clean, child_clean, prob=p0,\n",
    "                               label=lab, var=var, state=state)\n",
    "                if v not in seen:\n",
    "                    seen.add(v); q.append(v)\n",
    "\n",
    "    root_clean = strip_audits_from_history(root)\n",
    "    reach = {root_clean}\n",
    "    dq = deque([root_clean])\n",
    "    while dq:\n",
    "        x = dq.popleft()\n",
    "        for y in H.successors(x):\n",
    "            if y not in reach:\n",
    "                reach.add(y); dq.append(y)\n",
    "    H = H.subgraph(reach).copy()\n",
    "\n",
    "    for u in H.nodes():\n",
    "        succ = list(H.successors(u))\n",
    "        if succ:\n",
    "            s = sum(float(H[u][w]['prob']) for w in succ)\n",
    "            if s > 0:\n",
    "                for w in succ:\n",
    "                    H[u][w]['prob'] = float(H[u][w]['prob']) / s\n",
    "\n",
    "    return H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a7182ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invariants / sanity checks\n",
    "\n",
    "def assert_outflows_sum_to_1(G, tol=1e-12):\n",
    "    bad = []\n",
    "    for u in G.nodes():\n",
    "        succ = list(G.successors(u))\n",
    "        if not succ:\n",
    "            continue\n",
    "        s = sum(float(G[u][v]['prob']) for v in succ)\n",
    "        if not math.isclose(s, 1.0, rel_tol=0, abs_tol=tol):\n",
    "            bad.append((u, s))\n",
    "    assert not bad, f\"Outgoing probs not 1.0 at {len(bad)} nodes; first: {bad[:3]}\"\n",
    "\n",
    "def assert_unique_labels_per_floret(G):\n",
    "    bad = []\n",
    "    for u in G.nodes():\n",
    "        labs = [G[u][v]['label'] for v in G.successors(u)]\n",
    "        if len(labs) != len(set(labs)):\n",
    "            bad.append((u, labs))\n",
    "    assert not bad, f\"Duplicate labels in florets: {bad[:3]}\"\n",
    "\n",
    "def assert_same_marginals(model, G1, G2, var='H', tol=1e-9):\n",
    "    \"\"\"Placeholder: BN marginals should not depend on display order.\"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "084ad0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build both trees, compute positions, remove audits, check invariants\n",
    "\n",
    "def build_all():\n",
    "    G_hyp  = build_event_tree(MODEL, ORDER_HYPOTHESIS, VAR_STATES)\n",
    "    G_ch   = build_event_tree(MODEL, ORDER_CHRONO,  VAR_STATES,\n",
    "                              overrides=CHRONO_OVERRIDES)\n",
    "\n",
    "    for G in (G_hyp, G_ch):\n",
    "        assert_unique_labels_per_floret(G)\n",
    "        assert_outflows_sum_to_1(G)\n",
    "\n",
    "    st_hyp, pos_hyp = compute_positions(G_hyp)\n",
    "    st_ch,  pos_ch  = compute_positions(G_ch)\n",
    "\n",
    "    G_hyp_noaudit = eliminate_audits(G_hyp)\n",
    "    G_ch_noaudit  = eliminate_audits(G_ch)\n",
    "    for G in (G_hyp_noaudit, G_ch_noaudit):\n",
    "        assert_unique_labels_per_floret(G)\n",
    "        assert_outflows_sum_to_1(G)\n",
    "\n",
    "    return {\n",
    "        'hyp' : {'G':G_hyp, 'stage':st_hyp, 'pos':pos_hyp, 'noaudit':G_hyp_noaudit},\n",
    "        'chr' : {'G':G_ch,  'stage':st_ch,  'pos':pos_ch,  'noaudit':G_ch_noaudit},\n",
    "    }\n",
    "\n",
    "BUILDS = build_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43d5f429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting helpers \n",
    "\n",
    "def _tab20():\n",
    "    return [\n",
    "        \"#1f77b4\",\"#ff7f0e\",\"#2ca02c\",\"#d62728\",\"#9467bd\",\n",
    "        \"#8c564b\",\"#e377c2\",\"#7f7f7f\",\"#bcbd22\",\"#17becf\",\n",
    "        \"#aec7e8\",\"#ffbb78\",\"#98df8a\",\"#ff9896\",\"#c5b0d5\",\n",
    "        \"#c49c94\",\"#f7b6d2\",\"#c7c7c7\",\"#dbdb8d\",\"#9edae5\"\n",
    "    ]\n",
    "\n",
    "def stage_node_colors(G, stage_map):\n",
    "    pal = _tab20()\n",
    "    colors = {}\n",
    "    for n in G.nodes():\n",
    "        sid = stage_map[n]\n",
    "        colors[n] = pal[sid % len(pal)]\n",
    "    return colors\n",
    "\n",
    "def pos_node_colors(Q):\n",
    "    pal = _tab20()\n",
    "    colors = {}\n",
    "    for p in Q.nodes():\n",
    "        colors[p] = pal[int(p) % len(pal)]\n",
    "    return colors\n",
    "\n",
    "def _lerp(a, b, t):  # linear interpolate 0..1\n",
    "    return a + (b-a)*t\n",
    "\n",
    "def prob_to_hex(p):\n",
    "    c0 = (210, 224, 255)\n",
    "    c1 = ( 31,  78, 121)\n",
    "    r = int(_lerp(c0[0], c1[0], p))\n",
    "    g = int(_lerp(c0[1], c1[1], p))\n",
    "    b = int(_lerp(c0[2], c1[2], p))\n",
    "    return f\"#{r:02x}{g:02x}{b:02x}\"\n",
    "\n",
    "def _edge_text(p):\n",
    "    return f\"{p:.2f}\"\n",
    "\n",
    "def _labels_for_nodes(H):\n",
    "    lab = {}\n",
    "    for n in H.nodes():\n",
    "        if len(n) == 0:\n",
    "            lab[n] = \"Start\"\n",
    "        else:\n",
    "            var, state = parse_label(n[-1])\n",
    "            lab[n] = STATE_LABEL[var][state]\n",
    "    return lab\n",
    "\n",
    "# Graphviz (pydot) safe layout wrapper (kept; reimport guarded below)\n",
    "try:\n",
    "    from networkx.drawing.nx_pydot import graphviz_layout as _nx_graphviz_layout\n",
    "    _HAS_GV = True\n",
    "except Exception:\n",
    "    _HAS_GV = False\n",
    "\n",
    "def gv_layout(G, prog='dot', rankdir='LR'):\n",
    "    \"\"\"Compute Graphviz positions while avoiding ':' issues via relabeling.\"\"\"\n",
    "    import numpy as _np\n",
    "    import networkx as _nx\n",
    "\n",
    "    mapping = {n: f\"n{i}\" for i, n in enumerate(G.nodes())}\n",
    "\n",
    "    Hsafe = _nx.DiGraph()\n",
    "    Hsafe.add_nodes_from(mapping.values())\n",
    "    Hsafe.add_edges_from((mapping[u], mapping[v]) for u, v in G.edges())\n",
    "\n",
    "    Hsafe.graph.setdefault('graph', {})\n",
    "    Hsafe.graph['graph']['rankdir'] = rankdir\n",
    "\n",
    "    pos_safe = _nx_graphviz_layout(Hsafe, prog=prog)\n",
    "\n",
    "    inv = {v: k for k, v in mapping.items()}\n",
    "    return {inv[k]: _np.array(v, dtype=float) for k, v in pos_safe.items()}\n",
    "\n",
    "def _pretty_node_label(node):\n",
    "    if not node:\n",
    "        return \"Start\"\n",
    "    var, state = parse_label(node[-1])\n",
    "    return STATE_LABEL[var][state]\n",
    "\n",
    "def _pretty_edge_label(edge_label, p):\n",
    "    var, state = parse_label(edge_label)\n",
    "    return f\"{STATE_LABEL[var][state]}\\n{p:.2f}\"\n",
    "\n",
    "def _stage_palette(n):\n",
    "    cols = []\n",
    "    for i in range(n):\n",
    "        h = (i * 0.145) % 1.0\n",
    "        s = 0.45 + 0.25*((i % 2))\n",
    "        v = 0.92\n",
    "        r, g, b = colorsys.hsv_to_rgb(h, s, v)\n",
    "        cols.append(f\"#{int(r*255):02x}{int(g*255):02x}{int(b*255):02x}\")\n",
    "    return cols\n",
    "\n",
    "def draw_floret(model, history, next_var, figsize=(8,4)):\n",
    "    \"\"\"Plot a single floret P(next_var | history).\"\"\"\n",
    "    states = VAR_STATES[next_var]\n",
    "    probs  = [get_prob_bn(model, history, next_var, s) for s in states]\n",
    "    s = sum(probs); probs = [p/s for p in probs]\n",
    "    labels = [STATE_LABEL[next_var][s] for s in states]\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(\n",
    "        f\"Floret: {next_var} | \" +\n",
    "        \" â†’ \".join([STATE_LABEL[parse_label(h)[0]][parse_label(h)[1]] if ':' in h else h\n",
    "                          for h in history])\n",
    "    )\n",
    "\n",
    "    x = np.arange(len(states))\n",
    "    plt.bar(x, probs)\n",
    "    plt.xticks(x, labels, rotation=30, ha='right')\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.show()\n",
    "\n",
    "def draw_scenario_path(labels_with_p, title=\"Scenario path\"):\n",
    "    \"\"\"Draw a linear path with edge labels and probabilities.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 1.8))\n",
    "    ax.axis('off'); x=0.0\n",
    "    for lab, p in labels_with_p:\n",
    "        ax.add_patch(plt.Rectangle((x,0.2), 1.8, 0.6, fill=False))\n",
    "        ax.text(x+0.9, 0.5, lab, ha='center', va='center')\n",
    "        if p is not None:\n",
    "            ax.text(x+0.0, 0.9, f\"{p:.2f}\")\n",
    "        x += 2.0\n",
    "        ax.arrow(x-0.2, 0.5, 0.3, 0.0, head_width=0.05, head_length=0.1, length_includes_head=True)\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_staged_subtree_pdf(\n",
    "        G, stage_map, history_seq, pdf_path, title=\"\",\n",
    "        depth=None, rankdir='LR', a4='landscape',\n",
    "        *, min_edge_prob=0.0, mass_cut=0.0,\n",
    "        summarize_tail=False, abbreviate=False):\n",
    "    \"\"\"Render a staged subtree to PDF with optional pruning and tail summaries.\"\"\"\n",
    "\n",
    "    root = [n for n in G.nodes() if G.in_degree(n)==0][0]\n",
    "    u = root\n",
    "    for (var, st) in history_seq:\n",
    "        lab = canonical_label(var, st)\n",
    "        nxt = next((v for v in G.successors(u) if G[u][v]['label']==lab), None)\n",
    "        if nxt is None:\n",
    "            raise ValueError(f\"History not found: {history_seq}\")\n",
    "        u = nxt\n",
    "\n",
    "    keep = {u}; frontier=[u]; d=0\n",
    "    while frontier and (depth is None or d < depth):\n",
    "        nxt=[]\n",
    "        for x in frontier:\n",
    "            for y in G.successors(x):\n",
    "                keep.add(y); nxt.append(y)\n",
    "        frontier=nxt; d+=1\n",
    "    H = G.subgraph(keep).copy()\n",
    "\n",
    "    missing = [n for n in H.nodes() if n not in stage_map]\n",
    "    if missing:\n",
    "        raise ValueError(\"stage_map must be computed on the SAME graph you render.\")\n",
    "\n",
    "    def local_masses(Gsub, start):\n",
    "        mass = {start: 1.0}\n",
    "        for x in nx.topological_sort(Gsub):\n",
    "            mu = float(mass.get(x, 0.0))\n",
    "            for y in Gsub.successors(x):\n",
    "                mass[y] = mass.get(y, 0.0) + mu * float(Gsub[x][y]['prob'])\n",
    "        return mass\n",
    "    mass = local_masses(H, u)\n",
    "\n",
    "    kept_edges = []\n",
    "    tails = {}\n",
    "    for a, b, d in H.edges(data=True):\n",
    "        p = float(d['prob'])\n",
    "        if p < min_edge_prob:\n",
    "            if summarize_tail:\n",
    "                t = tails.setdefault(a, {'mass_sum':0.0, 'count':0})\n",
    "                t['mass_sum'] += mass.get(a,0.0) * p\n",
    "                t['count'] += 1\n",
    "            continue\n",
    "        flow = mass.get(a, 0.0) * p\n",
    "        if flow < mass_cut:\n",
    "            if summarize_tail:\n",
    "                t = tails.setdefault(a, {'mass_sum':0.0, 'count':0})\n",
    "                t['mass_sum'] += flow\n",
    "                t['count'] += 1\n",
    "            continue\n",
    "        kept_edges.append((a, b, d))\n",
    "\n",
    "    used_nodes = {u}\n",
    "    for a, b, _ in kept_edges:\n",
    "        used_nodes.add(a); used_nodes.add(b)\n",
    "\n",
    "    W, Hsize = ('11.69','8.27') if a4=='landscape' else ('8.27','11.69')\n",
    "    dot = Digraph(comment=title, format='pdf')\n",
    "    dot.attr(rankdir=rankdir, size=f'{W},{Hsize}!', margin='0.25',\n",
    "             nodesep='0.30', ranksep='0.60', splines='spline')\n",
    "\n",
    "    stages = sorted({stage_map[n] for n in used_nodes})\n",
    "    pal = {}\n",
    "    for i, s in enumerate(stages):\n",
    "        h = (i * 0.123) % 1.0\n",
    "        r, g, b = colorsys.hsv_to_rgb(h, 0.35, 1.0)\n",
    "        pal[s] = f\"#{int(r*255):02x}{int(g*255):02x}{int(b*255):02x}\"\n",
    "\n",
    "    leaf_override = {}\n",
    "    for n in used_nodes:\n",
    "        if H.out_degree(n) == 0 and n:\n",
    "            var, st = parse_label(n[-1])\n",
    "            if var == 'Hand':\n",
    "                leaf_override[n] = {'Victim':'#e9f8ef', 'Offender':'#fde9ea'}[st]\n",
    "\n",
    "    id_of = {n: f\"n{i}\" for i, n in enumerate(used_nodes)}\n",
    "\n",
    "    dot.attr('node', shape='circle', style='filled', penwidth='1.2',\n",
    "             color='#475e88', fontsize='12')\n",
    "    for n in used_nodes:\n",
    "        stid = stage_map[n]\n",
    "        fill = leaf_override.get(n, pal[stid])\n",
    "        label = \"Start\" if n == u else f\"s{stid}\"\n",
    "        dot.node(id_of[n], label, fillcolor=fill)\n",
    "\n",
    "    dot.attr('edge', color='#2f3b4a', penwidth='1.2', fontsize='11')\n",
    "    def _edge_label(lab, p):\n",
    "        var, st = parse_label(lab)\n",
    "        return f\"{STATE_LABEL[var][st]}\\n{p:.2f}\"\n",
    "    for a, b, d in kept_edges:\n",
    "        dot.edge(id_of[a], id_of[b], label=_edge_label(d['label'], float(d['prob'])))\n",
    "\n",
    "    if summarize_tail:\n",
    "        for a, info in tails.items():\n",
    "            if a not in used_nodes or info['count']==0 or info['mass_sum']<=0.0:\n",
    "                continue\n",
    "            tid = f\"tail_{id_of[a]}\"\n",
    "            dot.attr('node', shape='box', style='filled', fillcolor='#f1f1f1',\n",
    "                     color='#8c8c8c', fontsize='10')\n",
    "            dot.node(tid, f\"{info['count']} tiny branch(es)\\nSigma flow approx {info['mass_sum']:.2g}\")\n",
    "            dot.attr('edge', color='#a0a0a0', style='dashed')\n",
    "            dot.edge(id_of[a], tid, label=\"pruned\")\n",
    "\n",
    "    if title:\n",
    "        dot.attr(label=title, labelloc='t', fontsize='18')\n",
    "    dot.render(filename=pdf_path, cleanup=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information theory utilities \n",
    "\n",
    "def _entropy(pvec):\n",
    "    p = np.asarray(pvec, dtype=float)\n",
    "    p = p[p > 0]\n",
    "    return -float(np.sum(p * np.log2(p)))\n",
    "\n",
    "def _kl(p, q, eps=1e-12):\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    q = np.asarray(q, dtype=float)\n",
    "    p = np.clip(p, eps, 1.0)\n",
    "    q = np.clip(q, eps, 1.0)\n",
    "    p = p / p.sum(); q = q / q.sum()\n",
    "    return float(np.sum(p * (np.log2(p) - np.log2(q))))\n",
    "\n",
    "# Posterior helpers \n",
    "\n",
    "def _posterior_of(var, evidence):\n",
    "    \"\"\"Return a dict state->prob for a variable given evidence.\"\"\"\n",
    "    q = VE.query([var], evidence=evidence, show_progress=False)\n",
    "    states = MODEL.get_cpds(var).state_names[var]\n",
    "    return {s: float(q.values[i]) for i, s in enumerate(states)}\n",
    "\n",
    "def posterior_H(evidence=None):\n",
    "    evidence = evidence or {}\n",
    "    return _posterior_of('H', evidence)\n",
    "\n",
    "def _normalise_dict(d):\n",
    "    s = sum(d.values())\n",
    "    return {k: v/s for k, v in d.items()} if s > 0 else d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8a760fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected information gain (EIG) for a candidate test\n",
    "\n",
    "def expected_information_gain(candidate_var, given_evidence=None):\n",
    "    \"\"\"I(H; X | evidence) by enumeration over states of X.\"\"\"\n",
    "    given_evidence = given_evidence or {}\n",
    "\n",
    "    # Prior/posterior on H under current evidence\n",
    "    pH = posterior_H(given_evidence)\n",
    "    H_states = list(VAR_STATES['H'])\n",
    "    pH_vec = np.array([pH[h] for h in H_states], dtype=float)\n",
    "\n",
    "    # Distribution of candidate under evidence\n",
    "    qX = _posterior_of(candidate_var, given_evidence)\n",
    "    X_states = list(VAR_STATES[candidate_var])\n",
    "    qX_vec = np.array([qX[x] for x in X_states], dtype=float)\n",
    "\n",
    "    # Expected KL between H|X and H\n",
    "    eig = 0.0\n",
    "    for x in X_states:\n",
    "        ev = dict(given_evidence)\n",
    "        ev[candidate_var] = x\n",
    "        pHx = posterior_H(ev)\n",
    "        pHx_vec = np.array([pHx[h] for h in H_states], dtype=float)\n",
    "        eig += qX[x] * _kl(pHx_vec, pH_vec)\n",
    "    return float(eig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f243604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -CONFIG: variable names & labels\n",
    "VAR_H   = \"H\"\n",
    "VAR_O   = \"O\"\n",
    "VAR_DNA = \"DNA\"\n",
    "VAR_HAND= \"Hand\"\n",
    "VAR_BLD = \"Blood\"\n",
    "\n",
    "LABEL_O = (\"Absent\", \"Present\")            # order: 0=Absent, 1=Present\n",
    "LABEL_D = (\"Victim\", \"Offender\")           # order: 0=Victim, 1=Offender\n",
    "LABEL_HA = (\"Victim\", \"Offender\")          # for Hand (sinks)\n",
    "\n",
    "# tiny utilities \n",
    "def _factor_state_names(f):\n",
    "    # pgmpy Factor returned by infer.query\n",
    "    try:\n",
    "        return f.state_names\n",
    "    except AttributeError:\n",
    "        # pgmpy<0.1 fallback\n",
    "        return {v: list(range(f.get_cardinality([v])[v])) for v in f.scope()}\n",
    "\n",
    "def entropy_base2(p_dict):\n",
    "    p = np.array([p for p in p_dict.values()], dtype=float)\n",
    "    p = p[p > 0]\n",
    "    return -np.sum(p * np.log2(p))\n",
    "\n",
    "def marginal(infer, var, evidence=None):\n",
    "    q = infer.query(variables=[var], evidence=evidence, show_progress=False)\n",
    "    names = _factor_state_names(q)[var]\n",
    "    vals  = q.values\n",
    "    return {names[i]: float(vals[i]) for i in range(len(names))}\n",
    "\n",
    "# ---- drop-in replacements ----\n",
    "def joint2(infer, varx, vary, evidence=None):\n",
    "    q = infer.query(variables=[varx, vary], evidence=evidence, show_progress=False)\n",
    "    names = _factor_state_names(q)  # already defined in your notebook\n",
    "    xs = names[varx]; ys = names[vary]\n",
    "    order = list(q.variables)       # pgmpyâ€™s axis order (may differ)\n",
    "    ax_x = order.index(varx); ax_y = order.index(vary)\n",
    "    tbl = {}\n",
    "    for ix, x in enumerate(xs):\n",
    "        for iy, y in enumerate(ys):\n",
    "            if ax_x == 0 and ax_y == 1:\n",
    "                val = q.values[ix, iy]\n",
    "            else:\n",
    "                val = q.values[iy, ix]\n",
    "            tbl[(x, y)] = float(val)\n",
    "    return xs, ys, tbl\n",
    "\n",
    "def conditional_table(infer, child, parent, evidence=None, eps=0.0):\n",
    "    xs, ys, joint = joint2(infer, child, parent, evidence=evidence)\n",
    "    table = {}\n",
    "    for y in ys:\n",
    "        denom = sum(joint[(x, y)] for x in xs)\n",
    "        if denom <= eps:\n",
    "            table[y] = {x: float(\"nan\") for x in xs}\n",
    "        else:\n",
    "            table[y] = {x: joint[(x, y)]/denom for x in xs}\n",
    "    return xs, ys, table\n",
    "\n",
    "def mi_bits(infer, varx, vary, evidence=None):\n",
    "    xs, ys, joint = joint2(infer, varx, vary, evidence=evidence)\n",
    "    px = {x: sum(joint[(x,y)] for y in ys) for x in xs}\n",
    "    py = {y: sum(joint[(x,y)] for x in xs) for y in ys}\n",
    "    I = 0.0\n",
    "    for x in xs:\n",
    "        for y in ys:\n",
    "            pxy = joint[(x,y)]\n",
    "            if pxy > 0 and px[x] > 0 and py[y] > 0:\n",
    "                I += pxy * math.log2(pxy/(px[x]*py[y]))\n",
    "    return I\n",
    "\n",
    "def likelihood_ratios_binary(infer, test_var, H1, H0, evidence=None):\n",
    "    p1 = marginal(infer, test_var, {**(evidence or {}), VAR_H: H1})\n",
    "    p0 = marginal(infer, test_var, {**(evidence or {}), VAR_H: H0})\n",
    "    lr = {}\n",
    "    for t in p1:\n",
    "        lr[t] = p1[t] / p0[t] if p0[t] > 0 else float('inf')\n",
    "    return lr\n",
    "\n",
    "def print_audit_neutrality(infer, audit_var, e=None):\n",
    "    print(f\"Neutrality check for {audit_var} across H at e={e}\")\n",
    "    for h, _ in marginal(infer, VAR_H).items():\n",
    "        pa = marginal(infer, audit_var, {**(e or {}), VAR_H: h})\n",
    "        print(f\"P({audit_var} | H={h}) = { {k: round(v,4) for k,v in pa.items()} }\")\n",
    "\n",
    "\n",
    "def check_neutrality(infer, audit_var, target_var, e=None):\n",
    "    for t in infer.query([target_var], show_progress=False).state_names[target_var]:\n",
    "        q = infer.query([audit_var], evidence={**(e or {}), target_var: t}, show_progress=False)\n",
    "        print(f\"P({audit_var} | {target_var}={t}) = \"\n",
    "              f\"{ {s: round(float(p),3) for s,p in zip(q.state_names[audit_var], q.values)} }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty printers \n",
    "\n",
    "def _fmt_pct(x):\n",
    "    return f\"{100.0*float(x):.1f}%\"\n",
    "\n",
    "def print_sorted_posterior(title, dist):\n",
    "    print(\"\\n\" + title)\n",
    "    print(\"-\" * len(title))\n",
    "    for k, v in sorted(dist.items(), key=lambda kv: kv[1], reverse=True):\n",
    "        print(f\"{STATE_LABEL['H'][k]:28s}  {v: .4f} ({_fmt_pct(v)})\")\n",
    "\n",
    "def print_table(rows, headers=None, colwidth=14):\n",
    "    if headers:\n",
    "        print(\" \".join(h.ljust(colwidth) for h in headers))\n",
    "        print(\"-\" * (colwidth * len(headers)))\n",
    "    for r in rows:\n",
    "        print(\" \".join(str(c).ljust(colwidth) for c in r))\n",
    "\n",
    "def entropy_categorical(pdict):\n",
    "    return -sum(p*log2(p) for p in pdict.values() if p > 0)\n",
    "\n",
    "def print_entropy_H(infer, e=None):\n",
    "    post = infer.query([VAR_H], evidence=e, show_progress=False)\n",
    "    p_post = {state: float(prob) for state, prob in zip(post.state_names[VAR_H], post.values)}\n",
    "    H_post = entropy_categorical(p_post)\n",
    "    prior = infer.query([VAR_H], show_progress=False)\n",
    "    p_prior = {state: float(prob) for state, prob in zip(prior.state_names[VAR_H], prior.values)}\n",
    "    H_prior = entropy_categorical(p_prior)\n",
    "    print(f\"H(H) [bits]   : {H_prior:.3f}\")\n",
    "    print(f\"P(H | e={e}) : {p_post}\")\n",
    "    print(f\"H(H|e) [bits] : {H_post:.3f}\")\n",
    "    \n",
    "def print_dna_channel_given_O(infer, e=None):\n",
    "    # Get conditional with the engineâ€™s own state names\n",
    "    xs, ys, table = conditional_table(\n",
    "        infer, child=VAR_DNA, parent=VAR_O, evidence=e, eps=1e-15\n",
    "    )\n",
    "    def fmt(val):\n",
    "        if val is None:                 # missing key\n",
    "            return \"     â€”     \"\n",
    "        if isinstance(val, float) and math.isnan(val):  # undefined (zero denom)\n",
    "            return \"     â€”     \"\n",
    "        return f\"{val:12.4f}\"\n",
    "\n",
    "    # Header uses returned child labels (xs)\n",
    "    print(f\"P({VAR_DNA} | {VAR_O}, e={e})\")\n",
    "    print(\" \" * 13 + \"\".join(f\"{x:>12}\" for x in xs))\n",
    "    # Rows iterate over returned parent labels (ys)\n",
    "    for o in ys:\n",
    "        row = [table[o].get(x) for x in xs]\n",
    "        print(f\"{o:>12} \" + \" \".join(fmt(v) for v in row))\n",
    "\n",
    "def print_sink_masses(infer, e=None, hand_labels=LABEL_HA):\n",
    "    ph = marginal(infer, VAR_HAND, evidence=e)\n",
    "    off = ph.get(hand_labels[1], float(\"nan\"))  # Offender\n",
    "    vic = ph.get(hand_labels[0], float(\"nan\"))  # Victim\n",
    "    print(f\"Sinks (Hand report) at e={e}\")\n",
    "    print(f\"P(Offender sink | e) = {off:.6f}\")\n",
    "    print(f\"P(Victim   sink | e) = {vic:.6f}\")\n",
    "\n",
    "def print_q_and_MI(infer, e=None, present_label=LABEL_O[1]):\n",
    "    pO = marginal(infer, VAR_O, evidence=e)\n",
    "    q  = pO.get(present_label, float(\"nan\"))\n",
    "    I_dna = mi_bits(infer, VAR_DNA, VAR_O, evidence=e)\n",
    "    I_hand= mi_bits(infer, VAR_HAND, VAR_O, evidence=e)\n",
    "    print(f\"q(e)=P(O=Present | e={e}) = {q:.3f}\")\n",
    "    print(f\"I(DNA; O | e)   [bits]   = {I_dna:.3f}\")\n",
    "    print(f\"I(Hand; O | e)  [bits]   = {I_hand:.3f}\")\n",
    "    return q, I_dna, I_hand\n",
    "\n",
    "def compare_builds(infer_hyp, infer_chrono, e=None, tol=1e-6):\n",
    "    def snapshot(infer):\n",
    "        pO = marginal(infer, VAR_O, evidence=e)\n",
    "        q  = pO[LABEL_O[1]]\n",
    "        I  = mi_bits(infer, VAR_DNA, VAR_O, evidence=e)\n",
    "        ph = marginal(infer, VAR_HAND, evidence=e)\n",
    "        sink_off = ph[LABEL_HA[1]]\n",
    "        sink_vic = ph[LABEL_HA[0]]\n",
    "        return q, I, sink_off, sink_vic\n",
    "    a = snapshot(infer_hyp)\n",
    "    b = snapshot(infer_chrono)\n",
    "    names = [\"q(e)\", \"I(DNA;O|e)\", \"Offender sink\", \"Victim sink\"]\n",
    "    ok = True\n",
    "    for i, nm in enumerate(names):\n",
    "        if abs(a[i]-b[i]) > tol:\n",
    "            ok = False\n",
    "            print(f\"[DIFF] {nm}: hyp={a[i]:.6f}, chrono={b[i]:.6f}\")\n",
    "    if ok:\n",
    "        print(f\"[OK] Hypothesis-first vs Chronological are equal within tol={tol:g} on q, MI, and sink masses.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphviz render skipped: History not found: [('Lfound', 'Yes')]\n",
      "\n",
      "Table 3 â€” Posterior over H | Blood=Yes\n",
      "--------------------------------------\n",
      "Victim accidental              0.3959 (39.6%)\n",
      "Suicide                        0.2849 (28.5%)\n",
      "Offender self-defense          0.1188 (11.9%)\n",
      "Offender deliberate            0.1114 (11.1%)\n",
      "Offender accidental            0.0891 (8.9%)\n",
      "\n",
      "Next best test by EIG | Blood=Yes\n",
      "-------------------------------\n",
      "Best: Lfound  (EIG=0.1109 bits)\n",
      "Test           EIG(bits)     \n",
      "----------------------------\n",
      "Lfound         0.1109        \n",
      "Ltime          0.0755        \n",
      "O              0.0456        \n",
      "W              0.0404        \n",
      "DNA            0.0321        \n",
      "Hand           0.0000        \n",
      "BP             0.0000        \n",
      "AS             0.0000        \n",
      "AL             -0.0000       \n",
      "LR computation skipped: likelihood_ratios_binary() got an unexpected keyword argument 'given_evidence'\n",
      "\n",
      "Sensitivity â€” EIG(DNA;H | Blood=Yes): 0.0321 bits\n",
      "\n",
      "Conditional sink masses â€” P(Hand | Blood=Yes)\n",
      "--------------------------------------------\n",
      "Hand           Prob          \n",
      "----------------------------\n",
      "Victim         0.8851        \n",
      "Offender       0.1149        \n",
      "\n",
      "Summary â€” headline posteriors\n",
      "-----------------------------\n",
      "Prior argmax H: Victim accidental (0.460)\n",
      "Posterior argmax H | Blood=Yes: Victim accidental (0.396)\n",
      "H(H) [bits]   : 1.904\n",
      "P(H | e={'Blood': 'Yes'}) : {'O_delib': 0.11135533682001672, 'O_self': 0.11877631313352169, 'O_acc': 0.08908223485014126, 'V_acc': 0.3959215098984379, 'Suicide': 0.28486460529788254}\n",
      "H(H|e) [bits] : 2.074\n",
      "P(DNA | O, e={'Blood': 'Yes'})\n",
      "                        V           O\n",
      "      Absent       0.9800       0.0200\n",
      "     Present       0.1378       0.8622\n",
      "Sinks (Hand report) at e={'Blood': 'Yes'}\n",
      "P(Offender sink | e) = 0.114852\n",
      "P(Victim   sink | e) = 0.885148\n",
      "q(e)=P(O=Present | e={'Blood': 'Yes'}) = 0.548\n",
      "I(DNA; O | e)   [bits]   = 0.618\n",
      "I(Hand; O | e)  [bits]   = 0.001\n",
      "[OK] Hypothesis-first vs Chronological are equal within tol=1e-06 on q, MI, and sink masses.\n",
      "H(H) [bits]   : 1.904\n",
      "P(H | e={'Blood': 'Yes', 'Lfound': 'Yes'}) : {'O_delib': 0.09479426485776926, 'O_self': 0.07887997728895499, 'O_acc': 0.059159982966716235, 'V_acc': 0.43333144702147325, 'Suicide': 0.3338343278650863}\n",
      "H(H|e) [bits] : 1.904\n",
      "P(DNA | O, e={'Blood': 'Yes', 'Lfound': 'Yes'})\n",
      "                        V           O\n",
      "      Absent       0.9800       0.0200\n",
      "     Present       0.1368       0.8632\n",
      "Sinks (Hand report) at e={'Blood': 'Yes', 'Lfound': 'Yes'}\n",
      "P(Offender sink | e) = 0.114836\n",
      "P(Victim   sink | e) = 0.885164\n",
      "q(e)=P(O=Present | e={'Blood': 'Yes', 'Lfound': 'Yes'}) = 0.547\n",
      "I(DNA; O | e)   [bits]   = 0.620\n",
      "I(Hand; O | e)  [bits]   = 0.001\n",
      "Neutrality check for AS across H at e={'Blood': 'Yes', 'Lfound': 'Yes'}\n",
      "P(AS | H=O_delib) = {'Fail': 0.1, 'Pass': 0.9}\n",
      "P(AS | H=O_self) = {'Fail': 0.1, 'Pass': 0.9}\n",
      "P(AS | H=O_acc) = {'Fail': 0.1, 'Pass': 0.9}\n",
      "P(AS | H=V_acc) = {'Fail': 0.1, 'Pass': 0.9}\n",
      "P(AS | H=Suicide) = {'Fail': 0.1, 'Pass': 0.9}\n",
      "Neutrality check for AL across H at e={'Blood': 'Yes', 'Lfound': 'Yes'}\n",
      "P(AL | H=O_delib) = {'Fail': 0.1, 'Pass': 0.9}\n",
      "P(AL | H=O_self) = {'Fail': 0.1, 'Pass': 0.9}\n",
      "P(AL | H=O_acc) = {'Fail': 0.1, 'Pass': 0.9}\n",
      "P(AL | H=V_acc) = {'Fail': 0.1, 'Pass': 0.9}\n",
      "P(AL | H=Suicide) = {'Fail': 0.1, 'Pass': 0.9}\n",
      "[OK] Hypothesis-first vs Chronological are equal within tol=1e-06 on q, MI, and sink masses.\n",
      "\n",
      "Neutrality check for AL across H at e={'Blood': 'Yes', 'Lfound': 'Yes'}\n",
      "P(AL | H=O_delib) = {'Fail': 0.1, 'Pass': 0.9}\n",
      "P(AL | H=O_self) = {'Fail': 0.1, 'Pass': 0.9}\n",
      "P(AL | H=O_acc) = {'Fail': 0.1, 'Pass': 0.9}\n",
      "P(AL | H=V_acc) = {'Fail': 0.1, 'Pass': 0.9}\n",
      "P(AL | H=Suicide) = {'Fail': 0.1, 'Pass': 0.9}\n",
      "\n",
      "Neutrality check for AL across O at e={'Blood': 'Yes', 'Lfound': 'Yes'}\n",
      "P(AL | O=Absent) = {'Fail': 0.1, 'Pass': 0.9}\n",
      "P(AL | O=Present) = {'Fail': 0.1, 'Pass': 0.9}\n",
      "\n",
      "Neutrality check for AL across W at e={'Blood': 'Yes', 'Lfound': 'Yes'}\n",
      "P(AL | W=Victim) = {'Fail': 0.1, 'Pass': 0.9}\n",
      "P(AL | W=Offender) = {'Fail': 0.1, 'Pass': 0.9}\n",
      "\n",
      "Neutrality check for AS across H at e={'Blood': 'Yes', 'Lfound': 'Yes'}\n",
      "P(AS | H=O_delib) = {'Fail': 0.1, 'Pass': 0.9}\n",
      "P(AS | H=O_self) = {'Fail': 0.1, 'Pass': 0.9}\n",
      "P(AS | H=O_acc) = {'Fail': 0.1, 'Pass': 0.9}\n",
      "P(AS | H=V_acc) = {'Fail': 0.1, 'Pass': 0.9}\n",
      "P(AS | H=Suicide) = {'Fail': 0.1, 'Pass': 0.9}\n",
      "\n",
      "Neutrality check for AS across O at e={'Blood': 'Yes', 'Lfound': 'Yes'}\n",
      "P(AS | O=Absent) = {'Fail': 0.1, 'Pass': 0.9}\n",
      "P(AS | O=Present) = {'Fail': 0.1, 'Pass': 0.9}\n",
      "\n",
      "Neutrality check for AS across W at e={'Blood': 'Yes', 'Lfound': 'Yes'}\n",
      "P(AS | W=Victim) = {'Fail': 0.1, 'Pass': 0.9}\n",
      "P(AS | W=Offender) = {'Fail': 0.1, 'Pass': 0.9}\n"
     ]
    }
   ],
   "source": [
    "# Orchestration: a quick, reproducible end-to-end run \n",
    "\n",
    "EVIDENCE_BASE = {VAR_BLD: \"Yes\", \"Lfound\": \"Yes\"}\n",
    "\n",
    "def run_quick_demo():\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "    # 1) Build trees and compute overlays (already built globally)\n",
    "    builds = BUILDS\n",
    "    G_hyp, st_hyp = builds['hyp']['G'], builds['hyp']['stage']\n",
    "    G_chr, st_chr = builds['chr']['G'], builds['chr']['stage']\n",
    "\n",
    "    # 2) Render small staged subtrees (if graphviz available)\n",
    "    try:\n",
    "        render_staged_subtree_pdf(\n",
    "            G_hyp, st_hyp,\n",
    "            history_seq=[('Lfound','Yes')],\n",
    "            pdf_path=os.path.join('outputs','subtree_hyp_LfoundYes'),\n",
    "            title='Hypothesis order â€” subtree from Lfound=Yes',\n",
    "            depth=3, summarize_tail=True, min_edge_prob=0.02, mass_cut=1e-4\n",
    "        )\n",
    "        render_staged_subtree_pdf(\n",
    "            G_chr, st_chr,\n",
    "            history_seq=[('Lfound','Yes')],\n",
    "            pdf_path=os.path.join('outputs','subtree_chr_LfoundYes'),\n",
    "            title='Chronology order â€” subtree from Lfound=Yes',\n",
    "            depth=3, summarize_tail=True, min_edge_prob=0.02, mass_cut=1e-4\n",
    "        )\n",
    "        print(\"Rendered PDF subtrees in ./outputs/ (Graphviz).\")\n",
    "    except Exception as ex:\n",
    "        print(\"Graphviz render skipped:\", ex)\n",
    "\n",
    "    # 3) Key posterior with evidence Blood=Yes (mirrors your print index)\n",
    "    ev = {'Blood':'Yes'}\n",
    "    post_H_B = posterior_H(ev)\n",
    "    print_sorted_posterior(\"Table 3 â€” Posterior over H | Blood=Yes\", post_H_B)\n",
    "\n",
    "    # 4) Next best test by Expected Information Gain (EIG)\n",
    "    exclude = set(ev.keys())\n",
    "    candidates = [v for v in VAR_STATES.keys() if v not in exclude and v not in ('H',)]\n",
    "    eig_rows = []\n",
    "    for v in candidates:\n",
    "        try:\n",
    "            val = expected_information_gain(v, ev)\n",
    "            eig_rows.append((v, val))\n",
    "        except Exception:\n",
    "            pass\n",
    "    eig_rows.sort(key=lambda t: t[1], reverse=True)\n",
    "\n",
    "    print(\"\\nNext best test by EIG | Blood=Yes\")\n",
    "    print(\"-------------------------------\")\n",
    "    if eig_rows:\n",
    "        best_var, best_eig = eig_rows[0]\n",
    "        print(f\"Best: {best_var}  (EIG={best_eig:.4f} bits)\")\n",
    "    print_table([(v, f\"{e:.4f}\") for v, e in eig_rows], headers=[\"Test\",\"EIG(bits)\"])\n",
    "\n",
    "    # 5) Likelihood ratios for AS=Pass under Blood=Yes (Appendix C analogue)\n",
    "    try:\n",
    "        lrs = likelihood_ratios_binary('AS', 'Pass', given_evidence=ev)\n",
    "        rows = [(STATE_LABEL['H'][h], f\"{lrs[h]:.3f}\") for h in VAR_STATES['H']]\n",
    "        print(\"\\nAppendix C â€” LRs for AS=Pass | Blood=Yes (H vs ~H)\")\n",
    "        print(\"----------------------------------------------\")\n",
    "        print_table(rows, headers=[\"H\",\"LR\"])\n",
    "    except Exception as ex:\n",
    "        print(\"LR computation skipped:\", ex)\n",
    "\n",
    "    # 6) Sensitivity: EIG(DNA; H | Blood=Yes)\n",
    "    try:\n",
    "        eig_dna = expected_information_gain('DNA', ev)\n",
    "        print(f\"\\nSensitivity â€” EIG(DNA;H | Blood=Yes): {eig_dna:.4f} bits\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 7) Conditional sink masses: P(Hand | Blood=Yes)\n",
    "    try:\n",
    "        p_hand = _posterior_of('Hand', ev)\n",
    "        print(\"\\nConditional sink masses â€” P(Hand | Blood=Yes)\")\n",
    "        print(\"--------------------------------------------\")\n",
    "        print_table([(k, f\"{v:.4f}\") for k, v in p_hand.items()], headers=[\"Hand\",\"Prob\"]) \n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 8) Invariance & sanity checks already enforced; summarise headline\n",
    "    p0 = posterior_H()\n",
    "    p1 = post_H_B\n",
    "    def _top(d):\n",
    "        k = max(d, key=d.get)\n",
    "        return STATE_LABEL['H'][k], d[k]\n",
    "    h0, m0 = _top(p0)\n",
    "    h1, m1 = _top(p1)\n",
    "    print(\"\\nSummary â€” headline posteriors\")\n",
    "    print(\"-----------------------------\")\n",
    "    print(f\"Prior argmax H: {h0} ({m0:.3f})\")\n",
    "    print(f\"Posterior argmax H | Blood=Yes: {h1} ({m1:.3f})\")\n",
    "\n",
    "\n",
    "    infer_hyp    = VariableElimination(MODEL)\n",
    "    infer_chrono = VariableElimination(MODEL)\n",
    "\n",
    "    e = {VAR_BLD: \"Yes\"}  # base scene evidence used in the thesis\n",
    "\n",
    "    print_entropy_H(infer_hyp, e=e)                 # prints H(H) and H(H|e)\n",
    "    print_dna_channel_given_O(infer_hyp, e=e)       # prints the 2x2 scene-conditional DNA|O table\n",
    "    print_sink_masses(infer_hyp, e=e)               # prints Offender/Victim sink masses (Hand report)\n",
    "    print_q_and_MI(infer_hyp, e=e)                  # prints q(e), I(DNA;O|e), I(Hand;O|e)\n",
    "    compare_builds(infer_hyp, infer_chrono, e=e)    # asserts equality across builds (q, MI, sinks)\n",
    "\n",
    "    e = dict(EVIDENCE_BASE)  # {'Blood':'Yes','Lfound':'Yes'}\n",
    "\n",
    "    print_entropy_H(infer_hyp, e=e)\n",
    "    print_dna_channel_given_O(infer_hyp, e=e)          # now prints proper numbers (â‰ˆ 0.98 / 0.02 and â‰ˆ 0.14 / 0.86)\n",
    "    print_sink_masses(infer_hyp, e=e)\n",
    "    q, I_dna, I_hand = print_q_and_MI(infer_hyp, e=e)\n",
    "\n",
    "    # Optional: audit neutrality columns (baseline should be equal across H)\n",
    "    print_audit_neutrality(infer_hyp, audit_var=\"AS\", e=e)\n",
    "    print_audit_neutrality(infer_hyp, audit_var=\"AL\", e=e)\n",
    "\n",
    "    # Parity across builds\n",
    "    compare_builds(infer_hyp, infer_chrono, e=e)\n",
    "\n",
    "\n",
    "    # Run neutrality checks for AS and AL across H, O, and W \n",
    "    for a in sorted(AUDIT_VARS):          # deterministic order\n",
    "        for t in ['H','O','W']:           # fixed order for readability\n",
    "            print(f\"\\nNeutrality check for {a} across {t} at e={EVIDENCE_BASE}\")\n",
    "            check_neutrality(infer_hyp, audit_var=a, target_var=t, e=EVIDENCE_BASE)\n",
    "\n",
    "# Script entrypoint \n",
    "if __name__ == '__main__':\n",
    "    run_quick_demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509972e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
